{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "Load results files and run analysis on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import utils.utils as utils\n",
    "from utils.plotting import plot_links, find_linked_variables\n",
    "\n",
    "from utils.setup import Setup\n",
    "\n",
    "argv = sys.argv[1:]\n",
    "# argv = ['-c', 'cfg_pipeline.yml']\n",
    "# argv = ['-c', 'cfg_concat_lon120.yml']\n",
    "# argv = ['-c', 'cfg_single_lon120.yml']\n",
    "argv = [\"-c\", \"cfg_testing.yml\"]\n",
    "\n",
    "setup = Setup(argv)\n",
    "\n",
    "Path(setup.plots_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents_from_links(links):\n",
    "    \"\"\" Return a list of variables that are parents, given a\n",
    "    dictionary of links of the form {variable : [(parent, lag)]}\n",
    "    \"\"\"\n",
    "\n",
    "    linked_variables = set() # Avoids duplicates and sorts when iterated\n",
    "    for parents_list in links.values():\n",
    "        if len(parents_list) > 0:\n",
    "            for parent in parents_list:\n",
    "                linked_variables.add(parent[0]) # Remove the lag\n",
    "    return [(i in linked_variables)  for i in range(len(links))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_error(file, error_type, errors):\n",
    "    \"\"\"Record if a file has failed with a given error type\"\"\"\n",
    "\n",
    "    error_list = errors.get(error_type, list())\n",
    "    if file not in error_list:\n",
    "        error_list.append(results_file)\n",
    "        errors[error_type] = error_list\n",
    "\n",
    "\n",
    "def collect_in_dict(_dict, key, value):\n",
    "    \"\"\"Add a value to a list in a dictionary\"\"\"\n",
    "\n",
    "    collected_values = _dict.get(key, list())\n",
    "    collected_values.append(value)\n",
    "    _dict[key] = collected_values    \n",
    "\n",
    "\n",
    "def collect_results_file(key, results_file, collected_results, errors):\n",
    "    \"\"\"Collect pcmci results from a file in a dictionary with other\n",
    "    results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    key\n",
    "        Identifier of the variable and level to which the results will\n",
    "        be collected.\n",
    "    results_file\n",
    "        Path to a file storing results from a pcmci analysis.\n",
    "    collected_results\n",
    "        Dictionary. It is updated with the results read when this\n",
    "        function is called.\n",
    "    errors\n",
    "        Auxiliary dictionary to keep track of errors found during the\n",
    "        execution. It is updated whenever an error is found in a call.\n",
    "    \"\"\"\n",
    "\n",
    "    collected_pc_alpha = collected_results.get(key, dict())\n",
    "    if not results_file.is_file():\n",
    "        record_error(results_file, \"not_found\", errors)\n",
    "        return\n",
    "    results = utils.load_results(results_file)\n",
    "    for pc_alpha, alpha_result in results.items():\n",
    "        if len(alpha_result) > 0:\n",
    "            collected = collected_pc_alpha.get(pc_alpha, dict())\n",
    "            collected_pc_alpha[pc_alpha] = collected\n",
    "            # Collect parents\n",
    "            links = alpha_result[\"links\"]\n",
    "            parents = get_parents_from_links(links)\n",
    "            collect_in_dict(collected, \"parents\", parents)\n",
    "            # Collect val_matrix\n",
    "            val_matrix = alpha_result[\"val_matrix\"]\n",
    "            collect_in_dict(collected, \"val_matrix\", val_matrix)\n",
    "            # Check var_names\n",
    "            var_names = alpha_result[\"var_names\"]\n",
    "            if \"var_names\" not in collected:\n",
    "                collected[\"var_names\"] = var_names\n",
    "            else:\n",
    "                stored_var_names = collected[\"var_names\"]\n",
    "                assert stored_var_names == var_names, (\n",
    "                    \"Found different variable names.\\n\"\n",
    "                    f\"Expected {stored_var_names}\\nFound {var_names}\"\n",
    "                )\n",
    "        else:\n",
    "            record_error(results_file, \"is_empty\", errors)\n",
    "    collected_results[key] = collected_pc_alpha\n",
    "    # Doesn't return, instead modifies the received `collected results` object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load collected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_variables(var_children, levels):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for child in var_children:\n",
    "        \n",
    "        if child.dimensions == 2:\n",
    "            total += 1\n",
    "        elif child.dimensions == 3:\n",
    "            total += len(levels)\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.variable import Variable\n",
    "\n",
    "\n",
    "def collect_results(setup):\n",
    "    \"\"\"Collect the pcmci results defined in the setup in a file\"\"\"\n",
    "    collected_results = dict()\n",
    "    errors = dict()\n",
    "\n",
    "    total_vars = count_total_variables(setup.spcam_outputs, setup.children_idx_levs)\n",
    "    total_files = total_vars * len(setup.gridpoints)\n",
    "    file_progress = 0\n",
    "    step_progress = 0\n",
    "    step = 5\n",
    "\n",
    "    for child in setup.spcam_outputs:\n",
    "        print(f\"Variable: {child.name}\")\n",
    "        if child.dimensions == 2:\n",
    "            child_levels = [[setup.levels[-1], 0]]\n",
    "            child_var = Variable(child, None, None)\n",
    "        elif child.dimensions == 3:\n",
    "            child_levels = setup.children_idx_levs\n",
    "        for level in child_levels:\n",
    "            if child.dimensions == 3:\n",
    "                child_var = Variable(child, level[0], level[1])\n",
    "            if setup.analysis == \"single\":\n",
    "                for i_grid, (lat, lon) in enumerate(setup.gridpoints):\n",
    "                    results_file = utils.generate_results_filename_single(\n",
    "                        child,\n",
    "                        level[1],\n",
    "                        lat,\n",
    "                        lon,\n",
    "                        setup.ind_test_name,\n",
    "                        setup.experiment,\n",
    "                        setup.output_file_pattern,\n",
    "                        setup.output_folder,\n",
    "                    )\n",
    "                    collect_results_file(\n",
    "                        child_var, results_file, collected_results, errors\n",
    "                    )\n",
    "                    file_progress += 1\n",
    "                    if file_progress == total_files or (\n",
    "                        (file_progress / total_files * 100) >= step * step_progress\n",
    "                    ):\n",
    "                        step_progress = 1 + np.floor(\n",
    "                            (file_progress / total_files * 100) / step\n",
    "                        )\n",
    "                        print(\n",
    "                            \"Progress: {:.2f}% - {} of {} files\".format(\n",
    "                                file_progress / total_files * 100,\n",
    "                                file_progress,\n",
    "                                total_files,\n",
    "                            )\n",
    "                        )\n",
    "            elif setup.analysis == \"concat\":\n",
    "                results_file = utils.generate_results_filename_concat(\n",
    "                    child,\n",
    "                    level[-1],\n",
    "                    setup.gridpoints,\n",
    "                    setup.ind_test_name,\n",
    "                    setup.experiment,\n",
    "                    setup.output_file_pattern,\n",
    "                    setup.output_folder,\n",
    "                )\n",
    "                collect_results_file(key, results_file, collected_results, errors)\n",
    "\n",
    "    return collected_results, errors\n",
    "\n",
    "collected_results, errors = collect_results(setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print errors found loading collected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_errors(errors):\n",
    "    \"\"\"Prints the error dictionary according to type\"\"\"\n",
    "    \n",
    "    print(\"ERRORS\\n======\")\n",
    "    for error_type, error_list in errors.items():\n",
    "        msg = \"{}: {} of {} files ({:.2f}%)\".format(\n",
    "                error_type,\n",
    "                len(error_list),\n",
    "                total_files,\n",
    "                len(error_list)/total_files*100\n",
    "        )\n",
    "        print(msg)\n",
    "        print(\"-\" * len(msg))\n",
    "        for file in error_list:\n",
    "            print(file)\n",
    "\n",
    "print_errors(errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze collected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(collected_results, setup):\n",
    "    # Configuration\n",
    "    thresholds = setup.thresholds\n",
    "    pc_alphas_filter = [str(a) for a in setup.pc_alphas]\n",
    "\n",
    "    # Combine results\n",
    "    aggregated_results = dict()\n",
    "    var_names_parents = None\n",
    "    for child, collected_pc_alpha in collected_results.items():\n",
    "        dict_pc_alpha_parents = dict()\n",
    "        for pc_alpha, collected in collected_pc_alpha.items():\n",
    "            dict_threshold_parents = dict()\n",
    "            if pc_alpha not in pc_alphas_filter:\n",
    "                continue  # Skip this pc alpha\n",
    "            val_matrix_list = np.array(collected[\"val_matrix\"])\n",
    "            val_matrix_mean = val_matrix_list.mean(axis=0)\n",
    "\n",
    "            var_names = collected[\"var_names\"]\n",
    "            if var_names_parents is None:\n",
    "                # NOTE: This assumes that the list has only one child, and\n",
    "                # that the child is last\n",
    "                var_names_parents = var_names[:-1]\n",
    "            else:\n",
    "                assert var_names_parents == var_names[:-1], (\n",
    "                    \"Found different variable names. \"\n",
    "                    f\"Expected {var_names_parents}\\nFound {var_names[:-1]}\"\n",
    "                )\n",
    "\n",
    "            parents_matrix = collected[\"parents\"]\n",
    "            parents_matrix = np.array(parents_matrix)\n",
    "            parents_percent = parents_matrix.mean(axis=0)\n",
    "            for threshold in thresholds:\n",
    "                parents_filtered = parents_percent >= threshold\n",
    "                parents = [\n",
    "                    i for i in range(len(parents_filtered)) if parents_filtered[i]\n",
    "                ]\n",
    "                dict_threshold_parents[str(threshold)] = parents\n",
    "            dict_pc_alpha_parents[pc_alpha] = {\n",
    "                \"parents\": dict_threshold_parents,\n",
    "                \"val_matrix\": val_matrix_mean,\n",
    "                \"parents_percent\": parents_percent,\n",
    "                \"var_names\": var_names,\n",
    "            }\n",
    "\n",
    "        aggregated_results[child] = dict_pc_alpha_parents\n",
    "    return aggregated_results, var_names_parents\n",
    "\n",
    "\n",
    "aggregated_results, var_names_parents = aggregate_results(collected_results, setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save intermediary results for NN_Creation\n",
    "We can look into deleting this and do it on the fly in NN_Creation (takes a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO? DELETE\n",
    "utils.save_results(\n",
    "    aggregated_results,\n",
    "    \"./aggregated_results/{}\".format(\n",
    "        setup.yml_filename.replace(\".yml\", \".obj\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_aggregated_results(aggregated_results):\n",
    "    \"\"\"Print combined results\"\"\"\n",
    "\n",
    "    var_names_np = np.array(var_names_parents)\n",
    "    for child, dict_pc_alpha_parents in aggregated_results.items():\n",
    "        print(f\"\\n{child}\")\n",
    "        for pc_alpha, pc_alpha_results in dict_pc_alpha_parents.items():\n",
    "            print(f\"pc_alpha = {pc_alpha}\")\n",
    "            dict_threshold_parents = pc_alpha_results[\"parents\"]\n",
    "            for threshold, parents in dict_threshold_parents.items():\n",
    "                print(f\"* Threshold {threshold}:\\t{var_names_np[parents]}\")\n",
    "\n",
    "print_aggregated_results(aggregated_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for each pc_alpha and threshold combination with all children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate results file for filter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pc_alpha_plot_matrices(var_names_parents, aggregated_results):\n",
    "    len_parents = len(var_names_parents)\n",
    "    len_total = len_parents + len(aggregated_results)\n",
    "\n",
    "    dict_full = dict()\n",
    "    for i_child, (child, dict_pc_alpha_parents) in enumerate(\n",
    "        aggregated_results.items()\n",
    "    ):\n",
    "        i_child = i_child + len_parents\n",
    "        for pc_alpha, pc_alpha_results in dict_pc_alpha_parents.items():\n",
    "            dict_pc_alpha_full = dict_full.get(pc_alpha, dict())\n",
    "            dict_full[pc_alpha] = dict_pc_alpha_full\n",
    "\n",
    "            # Build val_matrix (only from inputs to outputs)\n",
    "            pc_alpha_val_matrix = pc_alpha_results[\"val_matrix\"]\n",
    "            full_val_matrix = dict_pc_alpha_full.get(\n",
    "                \"val_matrix\", np.zeros((len_total, len_total, 1))\n",
    "            )\n",
    "            full_val_matrix[:len_parents, i_child, 0] = pc_alpha_val_matrix[\n",
    "                :len_parents, -1, 1\n",
    "            ]\n",
    "            dict_pc_alpha_full[\"val_matrix\"] = full_val_matrix\n",
    "\n",
    "            # Build link_width\n",
    "            parents_percent = pc_alpha_results[\"parents_percent\"]\n",
    "            link_width = dict_pc_alpha_full.get(\n",
    "                \"link_width\", np.zeros((len_total, len_total, 1))\n",
    "            )\n",
    "            link_width[:len_parents, i_child, 0] = parents_percent[:len_parents]\n",
    "            dict_pc_alpha_full[\"link_width\"] = link_width\n",
    "\n",
    "    return dict_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combination:\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self, pc_alpha, threshold):\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"a{self.pc_alpha}-t{self.threshold}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return str(self) == str(other)\n",
    "\n",
    "\n",
    "def build_plot_matrices(var_names_parents, aggregated_results):\n",
    "\n",
    "    var_names_children = list()\n",
    "\n",
    "    dict_combinations = dict()\n",
    "    len_parents = len(var_names_parents)\n",
    "    len_total = len_parents + len(aggregated_results)\n",
    "    for i_child, (child, dict_pc_alpha_parents) in enumerate(\n",
    "        aggregated_results.items()\n",
    "    ):\n",
    "        i_child = i_child + len_parents\n",
    "        var_names_children.append(child)\n",
    "        for pc_alpha, pc_alpha_results in dict_pc_alpha_parents.items():\n",
    "            dict_threshold_parents = pc_alpha_results[\"parents\"]\n",
    "            for threshold, parents in dict_threshold_parents.items():\n",
    "                key = Combination(pc_alpha, threshold)\n",
    "                combination_results = dict_combinations.get(key, dict())\n",
    "\n",
    "                # Build links\n",
    "                links = combination_results.get(\n",
    "                    \"links\", {i: [] for i in range(len_total)}\n",
    "                )\n",
    "                links[i_child] = [(parent, 0) for parent in parents]\n",
    "                combination_results[\"links\"] = links\n",
    "\n",
    "                # Store results\n",
    "                dict_combinations[key] = combination_results\n",
    "\n",
    "    all_var_names = var_names_parents + var_names_children  # Concatenation\n",
    "    pc_alpha_plot_matrices = build_pc_alpha_plot_matrices(\n",
    "        var_names_parents, aggregated_results\n",
    "    )\n",
    "    for combination, combination_results in dict_combinations.items():\n",
    "        combination_results[\"var_names\"] = all_var_names\n",
    "        val_matrix = pc_alpha_plot_matrices[combination.pc_alpha][\"val_matrix\"]\n",
    "        combination_results[\"val_matrix\"] = val_matrix\n",
    "        link_width = pc_alpha_plot_matrices[combination.pc_alpha][\"link_width\"]\n",
    "        combination_results[\"link_width\"] = link_width\n",
    "\n",
    "    return dict_combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_sizes(links):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    linked_variables = find_linked_variables(links)\n",
    "    n_linked_vars = len(linked_variables)\n",
    "    print(f\"n_linked_vars : {n_linked_vars}\")\n",
    "    if n_linked_vars <= 35:\n",
    "        # Small\n",
    "        figsize = (16, 16)\n",
    "        node_size = 0.15\n",
    "    elif n_linked_vars <= 70:\n",
    "        # Medium\n",
    "        figsize = (32, 32)\n",
    "        node_size = 0.10\n",
    "    else:\n",
    "        #Big\n",
    "        figsize = (48, 48)\n",
    "        node_size = 0.05\n",
    "    \n",
    "    return figsize, node_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_val_matrix(val_matrix):\n",
    "#     \"\"\"\n",
    "#     Scales to values between -1 and 1, keeping zero in the same place\n",
    "#     \"\"\"\n",
    "#     max_val = np.abs(val_matrix).max()\n",
    "#     return val_matrix/max_val\n",
    "\n",
    "\n",
    "def scale_link_width(o_link_width, threshold):\n",
    "    \"\"\"\n",
    "    Scales 0.05 (so links are drawn) and 1 range, taking the threshold\n",
    "    as the minimum\n",
    "    \"\"\"\n",
    "    min_val = threshold\n",
    "#     min_val = o_link_width[o_link_width >= threshold].min()\n",
    "    smallest_link = 0.05\n",
    "    link_width = o_link_width + smallest_link\n",
    "    link_width -= min_val\n",
    "    link_width[o_link_width < threshold] = 0 \n",
    "    return link_width/link_width.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_results(var_names_parents, aggregated_results):\n",
    "    dict_combinations = build_plot_matrices(var_names_parents, aggregated_results)\n",
    "\n",
    "    for combination, combination_results in dict_combinations.items():\n",
    "        print(combination)\n",
    "        plot_filename = \"{cfg}_{combination}.png\".format(\n",
    "            cfg=setup.yml_filename.rsplit(\".\")[0], combination=combination\n",
    "        )\n",
    "        plot_file = Path(setup.plots_folder, plot_filename)\n",
    "        if not setup.overwrite and plot_file.is_file():\n",
    "            print(f\"Found file {plot_file}, skipping.\")\n",
    "            continue  # Ignore this result\n",
    "        links = combination_results[\"links\"]\n",
    "        figsize, node_size = recommend_sizes(links)\n",
    "\n",
    "        var_names = combination_results[\"var_names\"]\n",
    "\n",
    "        val_matrix = combination_results[\"val_matrix\"]\n",
    "        edges_array = np.array([val_matrix.min(), val_matrix.max()])\n",
    "        vmin_edges = np.max(np.abs(edges_array)) * -1.0\n",
    "        vmax_edges = np.max(np.abs(edges_array))\n",
    "        edge_ticks = (\n",
    "            vmax_edges - vmin_edges\n",
    "        ) * 0.2  # So it has the same proportions as default\n",
    "        link_width = combination_results[\"link_width\"]\n",
    "        # val_matrix = scale_val_matrix(val_matrix)\n",
    "        link_width = scale_link_width(link_width, float(combination.threshold))\n",
    "\n",
    "        plot_links(\n",
    "            links,\n",
    "            var_names,\n",
    "            val_matrix=val_matrix,\n",
    "            vmin_edges=vmin_edges,\n",
    "            vmax_edges=vmax_edges,\n",
    "            edge_ticks=edge_ticks,\n",
    "            link_width=link_width,\n",
    "            arrow_linewidth=10,\n",
    "            save_name=plot_file,\n",
    "            figsize=figsize,\n",
    "            node_size=node_size,\n",
    "            show_colorbar=True,\n",
    "        )\n",
    "\n",
    "\n",
    "plot_aggregated_results(var_names_parents, aggregated_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tigramite",
   "language": "python",
   "name": "tigramite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
