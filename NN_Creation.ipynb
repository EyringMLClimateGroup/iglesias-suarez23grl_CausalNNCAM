{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils as utils\n",
    "example_aggregated = \"./aggregated_results/cfg_single_lon120.obj\"\n",
    "example_aggregated = \"./aggregated_results/cfg_testing.obj\"\n",
    "aggregated_results = utils.load_results(example_aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def dense_nn(input_shape, output_shape, hidden_layers, activation):\n",
    "    \"\"\"\n",
    "    Creates a dense NN in base of the parameters received\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for n_layer_nodes in hidden_layers:\n",
    "        model.add(Dense(n_layer_nodes, activation=activation))\n",
    "\n",
    "    model.add(Dense(output_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def fc_model(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    hidden_layers,\n",
    "    activation,\n",
    "    conservation_layer=False,\n",
    "    inp_sub=None,\n",
    "    inp_div=None,\n",
    "    norm_q=None,\n",
    "):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "\n",
    "    # First hidden layer\n",
    "    x = Dense(hidden_layers[0])(inp)\n",
    "    x = act_layer(activation)(x)\n",
    "\n",
    "    # Remaining hidden layers\n",
    "    for h in hidden_layers[1:]:\n",
    "        x = Dense(h)(x)\n",
    "        x = act_layer(activation)(x)\n",
    "\n",
    "    if conservation_layer:\n",
    "        x = SurRadLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        x = MassConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        out = EntConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "\n",
    "    else:\n",
    "        out = Dense(output_shape)(x)\n",
    "\n",
    "    return tf.keras.models.Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDescription:\n",
    "    def __init__(self, variable, pc_alpha, threshold, parents):\n",
    "        self.variable = variable\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.threshold = threshold\n",
    "        self.parents = parents\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_shape = len(parents)\n",
    "        input_shape = (input_shape,)\n",
    "        model = dense_nn(\n",
    "            input_shape=input_shape,\n",
    "            output_shape=1,  # Only one variable\n",
    "            hidden_layers=[32, 32, 32],\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.variable}: a{self.pc_alpha}-t{self.threshold}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return str(self) == str(other)\n",
    "\n",
    "\n",
    "model_descriptions = list()\n",
    "for variable, pc_alpha_dict in aggregated_results.items():\n",
    "    print(variable)\n",
    "    for pc_alpha, pc_alpha_results in pc_alpha_dict.items():\n",
    "        for threshold, parents in pc_alpha_results[\"parents\"].items():\n",
    "            model_description = ModelDescription(variable, pc_alpha, threshold, parents)\n",
    "            model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_description in model_descriptions:\n",
    "    print(model_description)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = \"year\"\n",
    "\n",
    "DATA_FOLDER = \"/work/bd1083/b309162/preprocessed_data\"\n",
    "if DATA_TYPE == \"year\":\n",
    "    TRAIN_FN    = \"002_train_1_year.nc\"\n",
    "    VALID_FN    = \"005_valid_1_year.nc\"\n",
    "elif DATA_TYPE == \"month\":\n",
    "    TRAIN_FN    = \"002_train_1_month.nc\"\n",
    "    VALID_FN    = \"000_valid_1_month.nc\"\n",
    "# NORM_FN     = \"000_norm.nc\"\n",
    "NORM_FN_PNAS = \"/pf/b/b309198/projects/causal_discovery/rasp-et-al/data/001_norm.nc\"\n",
    "\n",
    "# TODO? Improve this, perhaps using my enum\n",
    "INPUT_VARS  = \"QBP TBP VBP PS SOLIN SHFLX LHFLX\".split(\" \") \n",
    "OUTPUT_VARS = \"PHQ TPHYSTND FSNT FSNS FLNT FLNS PRECT\".split(\" \")\n",
    "\n",
    "INPUT_SUB = \"mean\"\n",
    "INPUT_DIV = \"maxrs\"\n",
    "OUT_SCALE_DICT_FN = (\n",
    "    \"/work/bd1179/b309198/causal_discovery/rasp-et-al/CBRAIN-CAM/\"\n",
    "    \"nn_config/scale_dicts/002_pnas_scaling.pkl\"\n",
    ")\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# TODO Confirm\n",
    "# VAR_CUT_OFF = {\"QBP\": 14, \"TBP\": 14}\n",
    "VAR_CUT_OFF = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.utils import load_pickle\n",
    "out_scale_dict = load_pickle(OUT_SCALE_DICT_FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import neural_networks.cbrain.data_generator\n",
    "import neural_networks.cbrain.normalization\n",
    "import neural_networks.cbrain.utils\n",
    "reload(neural_networks.cbrain.data_generator)\n",
    "reload(neural_networks.cbrain.utils)\n",
    "reload(neural_networks.cbrain.normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.data_generator import DataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    data_fn          = Path(DATA_FOLDER, TRAIN_FN),\n",
    "    input_vars       = INPUT_VARS,\n",
    "    output_vars      = OUTPUT_VARS,\n",
    "#     norm_fn          = Path(DATA_FOLDER, NORM_FN),\n",
    "    norm_fn          = NORM_FN_PNAS,\n",
    "    input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "    output_transform = out_scale_dict,\n",
    "    batch_size       = BATCH_SIZE,\n",
    "    shuffle          = True,\n",
    "    var_cut_off      = VAR_CUT_OFF\n",
    ")\n",
    "\n",
    "\n",
    "nlat=64\n",
    "nlon=128\n",
    "ngeo = nlat * nlon\n",
    "\n",
    "valid_gen = DataGenerator(\n",
    "        data_fn          = Path(DATA_FOLDER, VALID_FN),\n",
    "        input_vars       = INPUT_VARS,\n",
    "        output_vars      = OUTPUT_VARS,\n",
    "        norm_fn          = NORM_FN_PNAS,\n",
    "        input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "        output_transform = out_scale_dict,\n",
    "        batch_size       = ngeo,\n",
    "        shuffle          = False,\n",
    "        #xarray           = True,\n",
    "        var_cut_off      = VAR_CUT_OFF\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalNNCAM",
   "language": "python",
   "name": "causalnncam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
