{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network creation\n",
    "\n",
    "This script takes results from \"aggregated results\", and generates neural\n",
    "networks based on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils.setup import Setup\n",
    "\n",
    "argv           = sys.argv[1:]\n",
    "# argv           = ['-c', 'cfg_single_lon120.yml']\n",
    "argv = [\"-c\", \"cfg_testing.yml\"]\n",
    "\n",
    "setup = Setup(argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasp et al.'s CBRAIN `fc_model` is used here for reference. As we are\n",
    "doing a simpler version, I've opted to create our own, shorter function\n",
    "for NN creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def dense_nn(input_shape, output_shape, hidden_layers, activation):\n",
    "    \"\"\"\n",
    "    Creates a dense NN in base of the parameters received\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for n_layer_nodes in hidden_layers:\n",
    "        model.add(Dense(n_layer_nodes, activation=activation))\n",
    "\n",
    "    model.add(Dense(output_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def fc_model(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    hidden_layers,\n",
    "    activation,\n",
    "    conservation_layer=False,\n",
    "    inp_sub=None,\n",
    "    inp_div=None,\n",
    "    norm_q=None,\n",
    "):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "\n",
    "    # First hidden layer\n",
    "    x = Dense(hidden_layers[0])(inp)\n",
    "    x = act_layer(activation)(x)\n",
    "\n",
    "    # Remaining hidden layers\n",
    "    for h in hidden_layers[1:]:\n",
    "        x = Dense(h)(x)\n",
    "        x = act_layer(activation)(x)\n",
    "\n",
    "    if conservation_layer:\n",
    "        x = SurRadLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        x = MassConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        out = EntConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "\n",
    "    else:\n",
    "        out = Dense(output_shape)(x)\n",
    "\n",
    "    return tf.keras.models.Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.variable import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "# # NOTE: This will generate a lot of models, it may be better to put\n",
    "# # them on different folders\n",
    "# MODEL_FILENAME_PATTERN = \"model-{variable}-a{pc_alpha}-t{threshold}.h5\"\n",
    "\n",
    "\n",
    "class ModelDescription:\n",
    "    \"\"\"\n",
    "    Object that stores a Keras model and metainformation about it.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    output : Variable\n",
    "        Output variable of the model.\n",
    "    pc_alpha : str\n",
    "        Meta information. PC alpha used to find the parents.\n",
    "    threshold : str\n",
    "        Meta information. Gridpoint threshold used to select the parents.\n",
    "    parents : list(Variable)\n",
    "        List of the variables (and variable level) that cause the output\n",
    "        variable.\n",
    "    hidden_layers : list(int)\n",
    "        Description of the hidden dense layers of the model\n",
    "        (default [32, 32, 32]).\n",
    "    activation : Keras-compatible activation function\n",
    "        Activation function used for the hidden dense layers\n",
    "        (default \"relu\").\n",
    "    model : Keras model\n",
    "        Model created using the given information.\n",
    "        See `_build_model()`.\n",
    "    input_vars_dict:\n",
    "    \n",
    "    #TODO\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output,\n",
    "        parents,\n",
    "        model_type,\n",
    "        pc_alpha,\n",
    "        threshold,\n",
    "        setup\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : str\n",
    "            Output variable of the model in string format. See Variable.\n",
    "        parents : list(str)\n",
    "            List of strings for the variables that cause the output variable.\n",
    "            See Variable.\n",
    "        model_type : str\n",
    "            # TODO\n",
    "        pc_alpha : str\n",
    "            Meta information. PC alpha used to find the parents.\n",
    "        threshold : str\n",
    "            Meta information. Gridpoint threshold used to select the parents.\n",
    "        hidden_layers : list(int)\n",
    "            Description of the hidden dense layers of the model.\n",
    "        activation : Keras-compatible activation function\n",
    "            Activation function used for the hidden dense layers.\n",
    "        \"\"\"\n",
    "        self.setup = setup\n",
    "        self.output = Variable.parse_var_name(output)\n",
    "        parents = [Variable.parse_var_name(p) for p in parents]\n",
    "        self.parents = sorted(parents, key=lambda x: self.setup.input_order_list.index(x))\n",
    "        self.model_type = model_type\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.threshold = threshold\n",
    "        self.model = self._build_model()\n",
    "        self.input_vars_dict = ModelDescription._build_vars_dict(self.parents)\n",
    "        self.output_vars_dict = ModelDescription._build_vars_dict([self.output])\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Keras model with the given information.\n",
    "        \n",
    "        Some parameters are not configurable, taken from Rasp et al.\n",
    "        \"\"\"\n",
    "        input_shape = len(parents)\n",
    "        input_shape = (input_shape,)\n",
    "        model = dense_nn(\n",
    "            input_shape=input_shape,\n",
    "            output_shape=1,  # Only one output per model\n",
    "            hidden_layers=self.setup.hidden_layers,\n",
    "            activation=self.setup.activation,\n",
    "        )\n",
    "        model.compile(\n",
    "            # TODO? Move to configuration\n",
    "            optimizer = \"adam\", # From train.py (default)\n",
    "            loss = \"mse\", # From 006_8col_pnas_exact.yml\n",
    "            metrics = [tf.keras.losses.mse], # From train.py (default)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_vars_dict(list_variables):\n",
    "        \"\"\"\n",
    "        Convert the given list of Variable into a dictionary to be used\n",
    "        on the data generator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        list_variables : list(Variable)\n",
    "            List of variables to be converted to the dictionary format\n",
    "            used by the data generator\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        vars_dict : dict{str : list(int)}\n",
    "            Dictionary of the form {ds_name : list of levels}, where\n",
    "            \"ds_name\" is the name of the variable as stored in the\n",
    "            dataset, and \"list of levels\" a list containing the indices\n",
    "            of the levels of that variable to use, or None for 2D\n",
    "            variables.\n",
    "        \"\"\"\n",
    "        vars_dict = dict()\n",
    "        for variable in list_variables:\n",
    "            ds_name = variable.var.ds_name # Name used in the dataset\n",
    "            if variable.var.dimensions == 2:\n",
    "                vars_dict[ds_name] = None\n",
    "            elif variable.var.dimensions == 3:\n",
    "                levels = vars_dict.get(ds_name, list())\n",
    "                levels.append(variable.level_idx)\n",
    "                vars_dict[ds_name] = levels\n",
    "        return vars_dict\n",
    "    \n",
    "    def fit_model(self, x, validation_data, epochs, callbacks, verbose = 1):\n",
    "        self.model.fit(\n",
    "            x = x,\n",
    "            validation_data = validation_data,\n",
    "            epochs = epochs,\n",
    "            callbacks = callbacks,\n",
    "            verbose = verbose,\n",
    "        )\n",
    "    \n",
    "    def get_path(self, base_path):\n",
    "        # TODO? Separate \"filename\" from full path, to use in Tensorboard\n",
    "        path = Path(base_path, self.model_type)\n",
    "        if self.model_type == \"CausalSingleNN\":\n",
    "            path = path / Path(\"a{pc_alpha}-t{threshold}/\".format(\n",
    "                pc_alpha = self.pc_alpha,\n",
    "                threshold = self.threshold\n",
    "            ))\n",
    "        str_hl = str(self.setup.hidden_layers).replace(\", \", \"_\")\n",
    "        str_hl = str_hl.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        path = path / Path(\"hl_{hidden_layers}-act_{activation}-e_{epochs}/\".format(\n",
    "            hidden_layers = str_hl,\n",
    "            activation = self.setup.activation,\n",
    "            epochs = self.setup.epochs\n",
    "        ))\n",
    "        return path\n",
    "    \n",
    "    def get_filename(self):\n",
    "        i_var = setup.output_order.index(self.output.var)\n",
    "        i_level = self.output.level_idx\n",
    "        if i_level is None:\n",
    "            i_level = 0\n",
    "        return f\"{i_var}_{i_level}\"\n",
    "    \n",
    "    def save_model(self, base_path):\n",
    "        folder = self.get_path(base_path)\n",
    "        filename = self.get_filename()\n",
    "        print(f\"Using filename {filename}.\")\n",
    "        # Save model\n",
    "        self.model.save(Path(folder, f\"{filename}_model.h5\"))\n",
    "        # Save weights\n",
    "        self.model.save_weights(Path(folder, f\"{filename}_weights.h5\"))\n",
    "        # Save input list\n",
    "        self.save_input_list(folder, filename)\n",
    "        \n",
    "    def save_input_list(self, folder, filename):\n",
    "        input_list = self.get_input_list()\n",
    "        with open(Path(folder, f\"{filename}_input_list.txt\"), \"w\") as f:\n",
    "            for line in input_list:\n",
    "                print(str(line), file = f)\n",
    "        \n",
    "    def get_input_list(self):\n",
    "        return [int(var in self.parents) for var in setup.input_order_list]\n",
    "\n",
    "    def __str__(self):\n",
    "        name = f\"{self.model_type}: {self.output}\"\n",
    "        if self.pc_alpha != None:\n",
    "            # pc_alpha and threshold should be either both None or both not None\n",
    "            name += f\", a{self.pc_alpha}-t{self.threshold}\"\n",
    "        return name\n",
    "#         return self.get_filename()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return str(self) == str(other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descriptions = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read aggregated results and generate CausalSingleNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, results are loaded from a previously created file. This was\n",
    "made for testing purposes, and we may want to replace it for an analysis.\n",
    "\n",
    "In that replacement, the idea would be to execute more or less the same\n",
    "as aggregate_results up to the moment the file is currently saved, and\n",
    "continue with that object (aggregated results) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SingleNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.do_single_nn:\n",
    "    from utils.constants import SPCAM_Vars\n",
    "    # TODO Only parents & children in configuration\n",
    "    parents = list() # TODO Parents and levels\n",
    "    for spcam_var in setup.var_parents:\n",
    "        if spcam_var.dimensions == 3:\n",
    "            for level, _ in setup.parents_idx_levs:\n",
    "                # There's enough info to build a Variable list\n",
    "                # However, it could be better to do a bigger reorganization\n",
    "                var_name = f\"{spcam_var.name}-{round(level, 2)}\"\n",
    "                parents.append(var_name)\n",
    "        elif spcam_var.dimensions == 2:\n",
    "            var_name = spcam_var.name\n",
    "            parents.append(var_name)\n",
    "    \n",
    "    output_list = list()\n",
    "    for spcam_var in setup.var_children:\n",
    "        if spcam_var.dimensions == 3:\n",
    "            for level, _ in setup.children_idx_levs:\n",
    "                # There's enough info to build a Variable list\n",
    "                # However, it could be better to do a bigger reorganization\n",
    "                var_name = f\"{spcam_var.name}-{round(level, 2)}\"\n",
    "        elif spcam_var.dimensions == 2:\n",
    "            var_name = spcam_var.name\n",
    "        output_list.append(var_name)    \n",
    "\n",
    "    for output in output_list:\n",
    "        model_description = ModelDescription(\n",
    "            output,\n",
    "            parents,\n",
    "            \"SingleNN\",\n",
    "            pc_alpha = None,\n",
    "            threshold = None,\n",
    "            setup = setup,\n",
    "        )\n",
    "        model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CausalSingleNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.do_causal_single_nn:\n",
    "    import utils.utils as utils\n",
    "    from pathlib import Path\n",
    "    example_aggregated = Path(\n",
    "        \"./aggregated_results\",\n",
    "        setup.yml_filename.replace(\".yml\", \".obj\")\n",
    "    )\n",
    "    # example_aggregated = \"./aggregated_results/cfg_testing.obj\"\n",
    "    aggregated_results = utils.load_results(example_aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.do_causal_single_nn:\n",
    "    for output, pc_alpha_dict in aggregated_results.items():\n",
    "        print(output)\n",
    "        if len(pc_alpha_dict) == 0:  # May be empty\n",
    "            # TODO How to approach this?\n",
    "            print(\"Empty results\")\n",
    "            pass\n",
    "        for pc_alpha, pc_alpha_results in pc_alpha_dict.items():\n",
    "            var_names = np.array(pc_alpha_results[\"var_names\"])\n",
    "            for threshold, parent_idxs in pc_alpha_results[\"parents\"].items():\n",
    "                parents = var_names[parent_idxs]\n",
    "                model_description = ModelDescription(\n",
    "                    output,\n",
    "                    parents,\n",
    "                    \"CausalSingleNN\",\n",
    "                    pc_alpha,\n",
    "                    threshold,\n",
    "                    setup = setup,\n",
    "                )\n",
    "                model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from neural_networks.cbrain.data_generator import DataGenerator\n",
    "from neural_networks.cbrain.utils import load_pickle\n",
    "\n",
    "out_scale_dict = load_pickle(Path(setup.out_scale_dict_folder, setup.out_scale_dict_fn))\n",
    "input_transform = (setup.input_sub, setup.input_div)\n",
    "\n",
    "def build_train_generator(input_vars_dict, output_vars_dict):\n",
    "    train_gen = DataGenerator(\n",
    "        data_fn          = Path(setup.train_data_folder, setup.train_data_fn),\n",
    "        input_vars_dict       = input_vars_dict,\n",
    "        output_vars_dict      = output_vars_dict,\n",
    "        # norm_fn          = Path(DATA_FOLDER, NORM_FN),\n",
    "        norm_fn          = Path(setup.normalization_folder, setup.normalization_fn),\n",
    "        input_transform  = input_transform,\n",
    "        output_transform = out_scale_dict,\n",
    "        batch_size       = setup.batch_size,\n",
    "        shuffle          = True, # This feature doesn't seem to work\n",
    "    )\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "nlat=64\n",
    "nlon=128\n",
    "ngeo = nlat * nlon\n",
    "\n",
    "def build_valid_generator(input_vars_dict, output_vars_dict):\n",
    "    valid_gen = DataGenerator(\n",
    "            data_fn          = Path(setup.train_data_folder, setup.valid_data_fn),\n",
    "            input_vars_dict       = input_vars_dict,\n",
    "            output_vars_dict      = output_vars_dict,\n",
    "            norm_fn          = Path(setup.normalization_folder, setup.normalization_fn),\n",
    "            input_transform  = input_transform,\n",
    "            output_transform = out_scale_dict,\n",
    "            batch_size       = ngeo,\n",
    "            shuffle          = False,\n",
    "            #xarray           = True,\n",
    "    )\n",
    "    return valid_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all models in the model list and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from neural_networks.cbrain.learning_rate_schedule import LRUpdate\n",
    "from neural_networks.cbrain.save_weights import save_norm\n",
    "\n",
    "for model_description in model_descriptions:\n",
    "    print(model_description)\n",
    "    \n",
    "    input_vars_dict = model_description.input_vars_dict\n",
    "    output_vars_dict = model_description.output_vars_dict\n",
    "    \n",
    "    # TODO Test that the data is being taken correctly\n",
    "    with build_train_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as train_gen, build_valid_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as valid_gen:\n",
    "        lrs = LearningRateScheduler(LRUpdate(\n",
    "            init_lr = setup.init_lr,\n",
    "            step = setup.step_lr,\n",
    "            divide = setup.divide_lr\n",
    "        ))\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=Path(\n",
    "                model_description.get_path(setup.tensorboard_folder),\n",
    "                model_description.get_filename()\n",
    "            ),\n",
    "            histogram_freq=0,\n",
    "            write_graph=True,\n",
    "            write_images=False,\n",
    "            update_freq=\"epoch\",\n",
    "            profile_batch=2,\n",
    "            embeddings_freq=0,\n",
    "            embeddings_metadata=None,\n",
    "        )\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=setup.train_patience\n",
    "        )\n",
    "        model_description.fit_model(\n",
    "            x = train_gen,\n",
    "            validation_data = valid_gen,\n",
    "            epochs = setup.epochs,\n",
    "            callbacks = [lrs, tensorboard, early_stop],\n",
    "            verbose = setup.train_verbose,\n",
    "        )\n",
    "        model_description.save_model(setup.nn_output_path)\n",
    "        # Better to do this after saving the model to\n",
    "        # avoid having to create the folder manually\n",
    "        save_norm( \n",
    "            input_transform = train_gen.input_transform,\n",
    "            output_transform = train_gen.output_transform, \n",
    "            save_dir = str(model_description.get_path(setup.nn_output_path)),\n",
    "            filename = model_description.get_filename()\n",
    "        )\n",
    "\n",
    "#         print(len(train_gen))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalNNCAM",
   "language": "python",
   "name": "causalnncam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
