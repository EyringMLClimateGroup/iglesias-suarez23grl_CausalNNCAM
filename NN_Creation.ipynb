{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network creation\n",
    "\n",
    "This script takes results from \"aggregated results\", and generates neural\n",
    "networks based on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "argv           = sys.argv[1:]\n",
    "# argv           = ['-c', 'cfg_single_lon120.yml']\n",
    "argv = [\"-c\", \"cfg_testing.yml\"]\n",
    "\n",
    "cfg_file = argv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasp et al.'s CBRAIN `fc_model` is used here for reference. As we are\n",
    "doing a simpler version, I've opted to create our own, shorter function\n",
    "for NN creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def dense_nn(input_shape, output_shape, hidden_layers, activation):\n",
    "    \"\"\"\n",
    "    Creates a dense NN in base of the parameters received\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for n_layer_nodes in hidden_layers:\n",
    "        model.add(Dense(n_layer_nodes, activation=activation))\n",
    "\n",
    "    model.add(Dense(output_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def fc_model(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    hidden_layers,\n",
    "    activation,\n",
    "    conservation_layer=False,\n",
    "    inp_sub=None,\n",
    "    inp_div=None,\n",
    "    norm_q=None,\n",
    "):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "\n",
    "    # First hidden layer\n",
    "    x = Dense(hidden_layers[0])(inp)\n",
    "    x = act_layer(activation)(x)\n",
    "\n",
    "    # Remaining hidden layers\n",
    "    for h in hidden_layers[1:]:\n",
    "        x = Dense(h)(x)\n",
    "        x = act_layer(activation)(x)\n",
    "\n",
    "    if conservation_layer:\n",
    "        x = SurRadLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        x = MassConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        out = EntConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "\n",
    "    else:\n",
    "        out = Dense(output_shape)(x)\n",
    "\n",
    "    return tf.keras.models.Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.constants import SPCAM_Vars, DATA_FOLDER, ANCIL_FILE\n",
    "import utils.utils as utils\n",
    "\n",
    "class Variable:\n",
    "    \"\"\"\n",
    "    Object that stores a SPCAM variable and one specific level.\n",
    "    \n",
    "    Both the level in hPa and its index in ancillaries are stored.\n",
    "    \n",
    "    The main way to create these objects is to use `parse_var_name()`.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    spcam_var : SPCAM_Vars\n",
    "        SPCAM_Vars object corresponding to the variable.\n",
    "    level : str\n",
    "        Altitude in hPa.\n",
    "    level_idx : int\n",
    "        Level index as found in the ancillaries file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spcam_var, level_altitude, level_idx):\n",
    "        self.var = spcam_var\n",
    "        self.level = level_altitude\n",
    "        self.level_idx = level_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_var_name(var_name):\n",
    "        \"\"\"\n",
    "        Parses a string of variable and name to a Variable object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            String that represents a variable and a level, with format:\n",
    "            \"{variable name}-{altitude in hPa}\"\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Variable\n",
    "            Variable object that contains the information referenced\n",
    "            in the string\n",
    "        \"\"\"\n",
    "        values = var_name.split(\"-\")\n",
    "        spcam_name = values[0]\n",
    "        dict_spcam_vars = {v.name: v for v in SPCAM_Vars}\n",
    "        spcam_var = dict_spcam_vars[spcam_name]\n",
    "\n",
    "        if spcam_var.dimensions == 2:\n",
    "            level_altitude = level_idx = None\n",
    "        elif spcam_var.dimensions == 3:\n",
    "            levels, _, _ = utils.read_ancilaries(Path(DATA_FOLDER, ANCIL_FILE))\n",
    "            level_altitude = float(values[1])\n",
    "            level_idx = utils.find_closest_value(levels, level_altitude)\n",
    "\n",
    "        return Variable(spcam_var, level_altitude, level_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.var.dimensions == 2:\n",
    "            return f\"{self.var.name}\"\n",
    "        elif self.var.dimensions == 3:\n",
    "            return f\"{self.var.name}-{self.level}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# # NOTE: This will generate a lot of models, it may be better to put\n",
    "# # them on different folders\n",
    "# MODEL_FILENAME_PATTERN = \"model-{variable}-a{pc_alpha}-t{threshold}.h5\"\n",
    "\n",
    "MODEL_FILENAME_PATTERN = \"models/a{pc_alpha}-t{threshold}/{output}.h5\" # Folders\n",
    "\n",
    "class ModelDescription:\n",
    "    \"\"\"\n",
    "    Object that stores a Keras model and metainformation about it.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    output : Variable\n",
    "        Output variable of the model.\n",
    "    pc_alpha : str\n",
    "        Meta information. PC alpha used to find the parents.\n",
    "    threshold : str\n",
    "        Meta information. Gridpoint threshold used to select the parents.\n",
    "    parents : list(Variable)\n",
    "        List of the variables (and variable level) that cause the output\n",
    "        variable.\n",
    "    hidden_layers : list(int)\n",
    "        Description of the hidden dense layers of the model\n",
    "        (default [32, 32, 32]).\n",
    "    activation : Keras-compatible activation function\n",
    "        Activation function used for the hidden dense layers\n",
    "        (default \"relu\").\n",
    "    model : Keras model\n",
    "        Model created using the given information.\n",
    "        See `_build_model()`.\n",
    "    input_vars_dict:\n",
    "    \n",
    "    #TODO\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output,\n",
    "        parents,\n",
    "        model_type,\n",
    "        pc_alpha,\n",
    "        threshold,\n",
    "        hidden_layers,\n",
    "        activation,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : str\n",
    "            Output variable of the model in string format. See Variable.\n",
    "        parents : list(str)\n",
    "            List of strings for the variables that cause the output variable.\n",
    "            See Variable.\n",
    "        model_type : str\n",
    "            \n",
    "        pc_alpha : str\n",
    "            Meta information. PC alpha used to find the parents.\n",
    "        threshold : str\n",
    "            Meta information. Gridpoint threshold used to select the parents.\n",
    "        hidden_layers : list(int)\n",
    "            Description of the hidden dense layers of the model.\n",
    "        activation : Keras-compatible activation function\n",
    "            Activation function used for the hidden dense layers.\n",
    "        \"\"\"\n",
    "        self.output = Variable.parse_var_name(output)\n",
    "        self.parents = [Variable.parse_var_name(p) for p in parents]\n",
    "        self.model_type = model_type\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.threshold = threshold\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.model = self._build_model()\n",
    "        self.input_vars_dict = ModelDescription._build_vars_dict(self.parents)\n",
    "        self.output_vars_dict = ModelDescription._build_vars_dict([self.output])\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Keras model with the given information.\n",
    "        \n",
    "        Some parameters are not configurable, taken from Rasp et al.\n",
    "        \"\"\"\n",
    "        input_shape = len(parents)\n",
    "        input_shape = (input_shape,)\n",
    "        model = dense_nn(\n",
    "            input_shape=input_shape,\n",
    "            output_shape=1,  # Only one output per model\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer = \"adam\", # From train.py (default)\n",
    "            loss = \"mse\", # From 006_8col_pnas_exact.yml\n",
    "            metrics = [tf.keras.losses.mse], # From train.py (default)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_vars_dict(list_variables):\n",
    "        \"\"\"\n",
    "        Convert the given list of Variable into a dictionary to be used\n",
    "        on the data generator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        list_variables : list(Variable)\n",
    "            List of variables to be converted to the dictionary format\n",
    "            used by the data generator\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        vars_dict : dict{str : list(int)}\n",
    "            Dictionary of the form {ds_name : list of levels}, where\n",
    "            \"ds_name\" is the name of the variable as stored in the\n",
    "            dataset, and \"list of levels\" a list containing the indices\n",
    "            of the levels of that variable to use, or None for 2D\n",
    "            variables.\n",
    "        \"\"\"\n",
    "        vars_dict = dict()\n",
    "        for variable in list_variables:\n",
    "            ds_name = variable.var.ds_name # Name used in the dataset\n",
    "            if variable.var.dimensions == 2:\n",
    "                vars_dict[ds_name] = None\n",
    "            elif variable.var.dimensions == 3:\n",
    "                levels = vars_dict.get(ds_name, list())\n",
    "                levels.append(variable.level_idx)\n",
    "                vars_dict[ds_name] = levels\n",
    "        return vars_dict\n",
    "    \n",
    "    def generate_filename(self):\n",
    "        return MODEL_FILENAME_PATTERN.format(\n",
    "            output = str(self.output),\n",
    "            pc_alpha = self.pc_alpha,\n",
    "            threshold = self.threshold\n",
    "        )\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save(self.generate_filename())\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.model_type}: {self.output}, a{self.pc_alpha}-t{self.threshold}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return str(self) == str(other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read aggregated results and generate CausalSingleNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, results are loaded from a previously created file. This was\n",
    "made for testing purposes, and we may want to replace it for an analysis.\n",
    "\n",
    "In that replacement, the idea would be to execute more or less the same\n",
    "as aggregate_results up to the moment the file is currently saved, and\n",
    "continue with that object (aggregated results) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Move to configuration\n",
    "HIDDEN_LAYERS = [32, 32, 32]  # Arbitrary, I choose them because it's a small NN\n",
    "ACTIVATION = \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils as utils\n",
    "from pathlib import Path\n",
    "example_aggregated = Path(\n",
    "    \"./aggregated_results\",\n",
    "    cfg_file.replace(\".yml\", \".obj\")\n",
    ")\n",
    "# example_aggregated = \"./aggregated_results/cfg_testing.obj\"\n",
    "aggregated_results = utils.load_results(example_aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descriptions = list()\n",
    "for output, pc_alpha_dict in aggregated_results.items():\n",
    "    print(output)\n",
    "    if len(pc_alpha_dict) == 0:  # May be empty\n",
    "        # TODO How to approach this?\n",
    "        print(\"Empty results\")\n",
    "        pass\n",
    "    for pc_alpha, pc_alpha_results in pc_alpha_dict.items():\n",
    "        var_names = np.array(pc_alpha_results[\"var_names\"])\n",
    "        for threshold, parent_idxs in pc_alpha_results[\"parents\"].items():\n",
    "            parents = var_names[parent_idxs]\n",
    "            model_description = ModelDescription(\n",
    "                output,\n",
    "                parents,\n",
    "                \"CausalSingleNN\",\n",
    "                pc_alpha,\n",
    "                threshold,\n",
    "                hidden_layers=HIDDEN_LAYERS,\n",
    "                activation=ACTIVATION\n",
    "            )\n",
    "            model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SingleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import SPCAM_Vars\n",
    "# TODO Only parents & children in configuration\n",
    "output_list = [] # TODO Outputs and levels\n",
    "parents = [] # TODO Parents and levels\n",
    "for output in output_list:\n",
    "    model_description = ModelDescription(\n",
    "        output,\n",
    "        parents,\n",
    "        \"CausalSingleNN\",\n",
    "        pc_alpha,\n",
    "        threshold,\n",
    "        hidden_layers=HIDDEN_LAYERS,\n",
    "        activation=ACTIVATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO? Move this to configuration & constants\n",
    "\n",
    "DATA_TYPE = \"year\"\n",
    "DATA_TYPE = \"month\" # For testing\n",
    "\n",
    "DATA_FOLDER = \"/work/bd1083/b309162/preprocessed_data\"\n",
    "if DATA_TYPE == \"year\":\n",
    "    TRAIN_FN    = \"002_train_1_year.nc\"\n",
    "    VALID_FN    = \"005_valid_1_year.nc\"\n",
    "elif DATA_TYPE == \"month\":\n",
    "    TRAIN_FN    = \"002_train_1_month.nc\"\n",
    "    VALID_FN    = \"000_valid_1_month.nc\"\n",
    "# NORM_FN     = \"000_norm.nc\"\n",
    "NORM_FN_PNAS = (\n",
    "    \"/pf/b/b309198/projects/causal_discovery/rasp-et-al/data/\"\n",
    "    \"001_norm.nc\"\n",
    ") # From Tom https://github.com/tbeucler/CBRAIN-CAM/tree/master/notebooks/tbeucler_devlog/UW_DATA\n",
    "\n",
    "INPUT_SUB = \"mean\" # From 006_8col_pnas_exact.yml\n",
    "INPUT_DIV = \"maxrs\" # From 006_8col_pnas_exact.yml\n",
    "OUT_SCALE_DICT_FN = ( # From 006_8col_pnas_exact.yml\n",
    "    \"/work/bd1179/b309198/causal_discovery/rasp-et-al/CBRAIN-CAM/\"\n",
    "    \"nn_config/scale_dicts/002_pnas_scaling.pkl\" \n",
    ") # From 006_8col_pnas_exact.yml\n",
    "BATCH_SIZE = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.utils import load_pickle\n",
    "out_scale_dict = load_pickle(OUT_SCALE_DICT_FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.data_generator import DataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "def build_train_generator(input_vars_dict, output_vars_dict):\n",
    "    train_gen = DataGenerator(\n",
    "        data_fn          = Path(DATA_FOLDER, TRAIN_FN),\n",
    "        input_vars_dict       = input_vars_dict,\n",
    "        output_vars_dict      = output_vars_dict,\n",
    "        # norm_fn          = Path(DATA_FOLDER, NORM_FN),\n",
    "        norm_fn          = NORM_FN_PNAS,\n",
    "        input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "        output_transform = out_scale_dict,\n",
    "        batch_size       = BATCH_SIZE,\n",
    "        shuffle          = True, # This feature doesn't seem to work\n",
    "    )\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "nlat=64\n",
    "nlon=128\n",
    "ngeo = nlat * nlon\n",
    "\n",
    "def build_valid_generator(input_vars_dict, output_vars_dict):\n",
    "    valid_gen = DataGenerator(\n",
    "            data_fn          = Path(DATA_FOLDER, VALID_FN),\n",
    "            input_vars_dict       = input_vars_dict,\n",
    "            output_vars_dict      = output_vars_dict,\n",
    "            norm_fn          = NORM_FN_PNAS,\n",
    "            input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "            output_transform = out_scale_dict,\n",
    "            batch_size       = ngeo,\n",
    "            shuffle          = False,\n",
    "            #xarray           = True,\n",
    "    )\n",
    "    return valid_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is only for developing purposes, to not have to restart the full\n",
    "Notebook every time one of these files was changed.\n",
    "\n",
    "Can be safely removed once they are stable.\n",
    "\"\"\"\n",
    "from importlib import reload\n",
    "import neural_networks.cbrain.data_generator\n",
    "import neural_networks.cbrain.normalization\n",
    "import neural_networks.cbrain.utils\n",
    "import neural_networks.cbrain.learning_rate_schedule\n",
    "reload(neural_networks.cbrain.data_generator)\n",
    "reload(neural_networks.cbrain.utils)\n",
    "reload(neural_networks.cbrain.normalization)\n",
    "reload(neural_networks.cbrain.learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all models in the model list and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from neural_networks.cbrain.learning_rate_schedule import LRUpdate\n",
    "\n",
    "for model_description in model_descriptions:\n",
    "    print(model_description)\n",
    "    input_vars_dict = model_description.input_vars_dict\n",
    "    output_vars_dict = model_description.output_vars_dict\n",
    "    \n",
    "    # TODO Test that the data is being taken correctly\n",
    "    with build_train_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as train_gen, build_valid_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as valid_gen:\n",
    "        lrs = LearningRateScheduler(LRUpdate(\n",
    "            init_lr = 0.001, # From train.py (default)\n",
    "            step = 1, # From 006_8col_pnas_exact.yml\n",
    "            divide = 5 # From train.py (default)\n",
    "        ))\n",
    "        model_description.model.fit(\n",
    "            x = train_gen,\n",
    "            validation_data = valid_gen,\n",
    "            epochs = 4, # From 006_8col_pnas_exact.yml\n",
    "            # epochs = 18, # TODO: Decide number of epochs, maybe use early stopper\n",
    "            # verbose = 0, # Silent\n",
    "            # verbose = 2, # Summary on epoch\n",
    "            callbacks = [lrs]\n",
    "        )\n",
    "        model_description.save_model()\n",
    "#         print(len(train_gen))\n",
    "        break # For testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalNNCAM",
   "language": "python",
   "name": "causalnncam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
