{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils as utils\n",
    "example_aggregated = \"./aggregated_results/cfg_single_lon120.obj\"\n",
    "example_aggregated = \"./aggregated_results/cfg_testing.obj\"\n",
    "aggregated_results = utils.load_results(example_aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def dense_nn(input_shape, output_shape, hidden_layers, activation):\n",
    "    \"\"\"\n",
    "    Creates a dense NN in base of the parameters received\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for n_layer_nodes in hidden_layers:\n",
    "        model.add(Dense(n_layer_nodes, activation=activation))\n",
    "\n",
    "    model.add(Dense(output_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def fc_model(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    hidden_layers,\n",
    "    activation,\n",
    "    conservation_layer=False,\n",
    "    inp_sub=None,\n",
    "    inp_div=None,\n",
    "    norm_q=None,\n",
    "):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "\n",
    "    # First hidden layer\n",
    "    x = Dense(hidden_layers[0])(inp)\n",
    "    x = act_layer(activation)(x)\n",
    "\n",
    "    # Remaining hidden layers\n",
    "    for h in hidden_layers[1:]:\n",
    "        x = Dense(h)(x)\n",
    "        x = act_layer(activation)(x)\n",
    "\n",
    "    if conservation_layer:\n",
    "        x = SurRadLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        x = MassConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        out = EntConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "\n",
    "    else:\n",
    "        out = Dense(output_shape)(x)\n",
    "\n",
    "    return tf.keras.models.Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.constants import SPCAM_Vars, DATA_FOLDER, ANCIL_FILE\n",
    "import utils.utils as utils\n",
    "\n",
    "\n",
    "class Variable:\n",
    "\n",
    "    \"\"\"\n",
    "    A combination of a SPCAM variable and one specific level, so the\n",
    "    information can be easily referred.\n",
    "    The variable is expressed as the corresponding SPCAM_Vars object.\n",
    "    The level is expressed as a pair: [altitude, index]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spcam_var, level_altitude, level_idx):\n",
    "        self.var = spcam_var\n",
    "        self.level = level_altitude\n",
    "        self.level_idx = level_idx\n",
    "\n",
    "    def parse_var_name(var_name):\n",
    "        \"\"\"\n",
    "        Parses a var_name string with format \"{name}-{altitude}\" to a\n",
    "        Variable object\n",
    "        \"\"\"\n",
    "        values = var_name.split(\"-\")\n",
    "        spcam_name = values[0]\n",
    "        dict_spcam_vars = {v.name: v for v in SPCAM_Vars}\n",
    "        spcam_var = dict_spcam_vars[spcam_name]\n",
    "\n",
    "        if spcam_var.dimensions == 2:\n",
    "            level_altitude = level_idx = None\n",
    "        elif spcam_var.dimensions == 3:\n",
    "            levels, _, _ = utils.read_ancilaries(Path(DATA_FOLDER, ANCIL_FILE))\n",
    "            level_altitude = float(values[1])\n",
    "            level_idx = utils.find_closest_value(levels, level_altitude)\n",
    "\n",
    "        return Variable(spcam_var, level_altitude, level_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.var.dimensions == 2:\n",
    "            return f\"{self.var.name}\"\n",
    "        elif self.var.dimensions == 3:\n",
    "            return f\"{self.var.name}-{self.level}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# NOTE: This will generate a lot of models, it may be better to put\n",
    "# them on different folders\n",
    "MODEL_FILENAME_PATTERN = \"model-{variable}-a{pc_alpha}-t{threshold}.h5\"\n",
    "MODEL_FILENAME_PATTERN = \"models/a{pc_alpha}-t{threshold}/{variable}.h5\" # Folders\n",
    "\n",
    "class ModelDescription:\n",
    "    def __init__(\n",
    "        self,\n",
    "        variable,\n",
    "        pc_alpha,\n",
    "        threshold,\n",
    "        parents,\n",
    "        hidden_layers = [32, 32, 32],\n",
    "        activation = \"relu\"\n",
    "    ):\n",
    "        self.variable = Variable.parse_var_name(variable)\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.threshold = threshold\n",
    "        self.parents = [Variable.parse_var_name(p) for p in parents]\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_shape = len(parents)\n",
    "        input_shape = (input_shape,)\n",
    "        model = dense_nn(\n",
    "            input_shape=input_shape,\n",
    "            output_shape=1,  # Only one variable\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer = \"adam\", # From train.py (default)\n",
    "            loss = \"mse\", # From 006_8col_pnas_exact.yml\n",
    "            metrics = [tf.keras.losses.mse], # From train.py (default)\n",
    "            \n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def generate_filename(self):\n",
    "        return MODEL_FILENAME_PATTERN.format(\n",
    "            variable = str(self.variable),\n",
    "            pc_alpha = self.pc_alpha,\n",
    "            threshold = self.threshold\n",
    "        )\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save(generate_filename())\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.variable}: a{self.pc_alpha}-t{self.threshold}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return str(self) == str(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descriptions = list()\n",
    "for variable, pc_alpha_dict in aggregated_results.items():\n",
    "    print(variable)\n",
    "    if len(pc_alpha_dict) == 0:  # May be empty\n",
    "        # TODO How to approach this?\n",
    "        print(\"Empty results\")\n",
    "        pass\n",
    "    for pc_alpha, pc_alpha_results in pc_alpha_dict.items():\n",
    "        var_names = np.array(pc_alpha_results[\"var_names\"])\n",
    "        for threshold, parent_idxs in pc_alpha_results[\"parents\"].items():\n",
    "            parents = var_names[parent_idxs]\n",
    "            model_description = ModelDescription(variable, pc_alpha, threshold, parents)\n",
    "            model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = \"year\"\n",
    "DATA_TYPE = \"month\" # For testing\n",
    "\n",
    "DATA_FOLDER = \"/work/bd1083/b309162/preprocessed_data\"\n",
    "if DATA_TYPE == \"year\":\n",
    "    TRAIN_FN    = \"002_train_1_year.nc\"\n",
    "    VALID_FN    = \"005_valid_1_year.nc\"\n",
    "elif DATA_TYPE == \"month\":\n",
    "    TRAIN_FN    = \"002_train_1_month.nc\"\n",
    "    VALID_FN    = \"000_valid_1_month.nc\"\n",
    "# NORM_FN     = \"000_norm.nc\"\n",
    "NORM_FN_PNAS = \"/pf/b/b309198/projects/causal_discovery/rasp-et-al/data/001_norm.nc\"\n",
    "\n",
    "# TODO? Improve this, perhaps using my enum\n",
    "INPUT_VARS  = \"QBP TBP VBP PS SOLIN SHFLX LHFLX\".split(\" \") \n",
    "OUTPUT_VARS = \"PHQ TPHYSTND FSNT FSNS FLNT FLNS PRECT\".split(\" \")\n",
    "\n",
    "INPUT_SUB = \"mean\"\n",
    "INPUT_DIV = \"maxrs\"\n",
    "OUT_SCALE_DICT_FN = (\n",
    "    \"/work/bd1179/b309198/causal_discovery/rasp-et-al/CBRAIN-CAM/\"\n",
    "    \"nn_config/scale_dicts/002_pnas_scaling.pkl\"\n",
    ")\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# TODO Confirm\n",
    "# VAR_CUT_OFF = {\"QBP\": 14, \"TBP\": 14}\n",
    "VAR_CUT_OFF = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.utils import load_pickle\n",
    "out_scale_dict = load_pickle(OUT_SCALE_DICT_FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.data_generator import DataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "def build_train_generator(input_vars_dict, output_vars_dict):\n",
    "    train_gen = DataGenerator(\n",
    "        data_fn          = Path(DATA_FOLDER, TRAIN_FN),\n",
    "        input_vars_dict       = input_vars_dict,\n",
    "        output_vars_dict      = output_vars_dict,\n",
    "    #     norm_fn          = Path(DATA_FOLDER, NORM_FN),\n",
    "        norm_fn          = NORM_FN_PNAS,\n",
    "        input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "        output_transform = out_scale_dict,\n",
    "        batch_size       = BATCH_SIZE,\n",
    "        shuffle          = True, # This feature doesn't seem to work\n",
    "    )\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "nlat=64\n",
    "nlon=128\n",
    "ngeo = nlat * nlon\n",
    "\n",
    "def build_valid_generator(input_vars_dict, output_vars_dict):\n",
    "    valid_gen = DataGenerator(\n",
    "            data_fn          = Path(DATA_FOLDER, VALID_FN),\n",
    "            input_vars_dict       = input_vars_dict,\n",
    "            output_vars_dict      = output_vars_dict,\n",
    "            norm_fn          = NORM_FN_PNAS,\n",
    "            input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "            output_transform = out_scale_dict,\n",
    "            batch_size       = ngeo,\n",
    "            shuffle          = False,\n",
    "            #xarray           = True,\n",
    "    )\n",
    "    return valid_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import neural_networks.cbrain.data_generator\n",
    "import neural_networks.cbrain.normalization\n",
    "import neural_networks.cbrain.utils\n",
    "import neural_networks.cbrain.learning_rate_schedule\n",
    "reload(neural_networks.cbrain.data_generator)\n",
    "reload(neural_networks.cbrain.utils)\n",
    "reload(neural_networks.cbrain.normalization)\n",
    "reload(neural_networks.cbrain.learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from neural_networks.cbrain.learning_rate_schedule import LRUpdate\n",
    "\n",
    "for model_description in model_descriptions:\n",
    "    print(model_description)\n",
    "    input_vars_dict = dict()\n",
    "    for parent in model_description.parents:\n",
    "        ds_name = parent.var.ds_name\n",
    "        if parent.var.dimensions == 2:\n",
    "            input_vars_dict[ds_name] = None\n",
    "        elif parent.var.dimensions == 3:\n",
    "            levels = input_vars_dict.get(ds_name, list())\n",
    "            levels.append(parent.level_idx)\n",
    "            input_vars_dict[ds_name] = levels\n",
    "    if model_description.variable.var.dimensions == 2:\n",
    "        level = None\n",
    "    elif model_description.variable.var.dimensions == 3:\n",
    "        level = [model_description.variable.level_idx]\n",
    "    output_vars_dict = {model_description.variable.var.ds_name: level}\n",
    "    \n",
    "    # TODO Test that the data is being taken correctly\n",
    "    with build_train_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as train_gen, build_valid_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as valid_gen:\n",
    "        # TODO fit model\n",
    "        lrs = LearningRateScheduler(LRUpdate(\n",
    "            init_lr = 0.001, # From train.py (default)\n",
    "            step = 1, # From 006_8col_pnas_exact.yml\n",
    "            divide = 5, # From train.py (default)\n",
    "        ))\n",
    "        model_description.model.fit(\n",
    "            x = train_gen,\n",
    "            validation_data = valid_gen,\n",
    "            epochs = 4, # From 006_8col_pnas_exact.yml\n",
    "            # epochs = 18, # TODO: Decide number of epochs, maybe use early stopper\n",
    "            # verbose = 0, # Silent\n",
    "            # verbose = 2, # Summary on epoch\n",
    "            callbacks = [lrs]\n",
    "        )\n",
    "        model_description.save_model()\n",
    "#         print(len(train_gen))\n",
    "        break # For testing\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalNNCAM",
   "language": "python",
   "name": "causalnncam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
