{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network creation\n",
    "\n",
    "This script takes results from \"aggregated results\", and generates neural\n",
    "networks based on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils.setup import Setup\n",
    "\n",
    "argv           = sys.argv[1:]\n",
    "# argv           = ['-c', 'cfg_single_lon120.yml']\n",
    "argv = [\"-c\", \"cfg_testing.yml\"]\n",
    "\n",
    "setup = Setup(argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasp et al.'s CBRAIN `fc_model` is used here for reference. As we are\n",
    "doing a simpler version, I've opted to create our own, shorter function\n",
    "for NN creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def dense_nn(input_shape, output_shape, hidden_layers, activation):\n",
    "    \"\"\"\n",
    "    Creates a dense NN in base of the parameters received\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for n_layer_nodes in hidden_layers:\n",
    "        model.add(Dense(n_layer_nodes, activation=activation))\n",
    "\n",
    "    model.add(Dense(output_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def fc_model(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    hidden_layers,\n",
    "    activation,\n",
    "    conservation_layer=False,\n",
    "    inp_sub=None,\n",
    "    inp_div=None,\n",
    "    norm_q=None,\n",
    "):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "\n",
    "    # First hidden layer\n",
    "    x = Dense(hidden_layers[0])(inp)\n",
    "    x = act_layer(activation)(x)\n",
    "\n",
    "    # Remaining hidden layers\n",
    "    for h in hidden_layers[1:]:\n",
    "        x = Dense(h)(x)\n",
    "        x = act_layer(activation)(x)\n",
    "\n",
    "    if conservation_layer:\n",
    "        x = SurRadLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        x = MassConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "        out = EntConsLayer(inp_sub, inp_div, norm_q)([inp, x])\n",
    "\n",
    "    else:\n",
    "        out = Dense(output_shape)(x)\n",
    "\n",
    "    return tf.keras.models.Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.constants import SPCAM_Vars, DATA_FOLDER, ANCIL_FILE\n",
    "import utils.utils as utils\n",
    "\n",
    "class Variable:\n",
    "    \"\"\"\n",
    "    Object that stores a SPCAM variable and one specific level.\n",
    "    \n",
    "    Both the level in hPa and its index in ancillaries are stored.\n",
    "    \n",
    "    The main way to create these objects is to use `parse_var_name()`.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    spcam_var : SPCAM_Vars\n",
    "        SPCAM_Vars object corresponding to the variable.\n",
    "    level : str\n",
    "        Altitude in hPa.\n",
    "    level_idx : int\n",
    "        Level index as found in the ancillaries file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spcam_var, level_altitude, level_idx):\n",
    "        self.var = spcam_var\n",
    "        self.level = level_altitude\n",
    "        if level_altitude is not None:\n",
    "            self.level = round(level_altitude, 2)\n",
    "        self.level_idx = level_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_var_name(var_name):\n",
    "        \"\"\"\n",
    "        Parses a string of variable and name to a Variable object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            String that represents a variable and a level, with format:\n",
    "            \"{variable name}-{altitude in hPa}\"\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Variable\n",
    "            Variable object that contains the information referenced\n",
    "            in the string\n",
    "        \"\"\"\n",
    "        values = var_name.split(\"-\")\n",
    "        spcam_name = values[0]\n",
    "        dict_spcam_vars = {v.name: v for v in SPCAM_Vars}\n",
    "        spcam_var = dict_spcam_vars[spcam_name]\n",
    "\n",
    "        if spcam_var.dimensions == 2:\n",
    "            level_altitude = level_idx = None\n",
    "        elif spcam_var.dimensions == 3:\n",
    "            levels, _, _ = utils.read_ancilaries(Path(DATA_FOLDER, ANCIL_FILE))\n",
    "            level_altitude = float(values[1])\n",
    "            level_idx = utils.find_closest_value(levels, level_altitude)\n",
    "\n",
    "        return Variable(spcam_var, level_altitude, level_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.var.dimensions == 2:\n",
    "            return f\"{self.var.name}\"\n",
    "        elif self.var.dimensions == 3:\n",
    "            return f\"{self.var.name}-{self.level}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if type(self) is type(other):\n",
    "            return self.var == other.var and self.level_idx == other.level_idx\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# # NOTE: This will generate a lot of models, it may be better to put\n",
    "# # them on different folders\n",
    "# MODEL_FILENAME_PATTERN = \"model-{variable}-a{pc_alpha}-t{threshold}.h5\"\n",
    "\n",
    "\n",
    "class ModelDescription:\n",
    "    \"\"\"\n",
    "    Object that stores a Keras model and metainformation about it.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    output : Variable\n",
    "        Output variable of the model.\n",
    "    pc_alpha : str\n",
    "        Meta information. PC alpha used to find the parents.\n",
    "    threshold : str\n",
    "        Meta information. Gridpoint threshold used to select the parents.\n",
    "    parents : list(Variable)\n",
    "        List of the variables (and variable level) that cause the output\n",
    "        variable.\n",
    "    hidden_layers : list(int)\n",
    "        Description of the hidden dense layers of the model\n",
    "        (default [32, 32, 32]).\n",
    "    activation : Keras-compatible activation function\n",
    "        Activation function used for the hidden dense layers\n",
    "        (default \"relu\").\n",
    "    model : Keras model\n",
    "        Model created using the given information.\n",
    "        See `_build_model()`.\n",
    "    input_vars_dict:\n",
    "    \n",
    "    #TODO\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output,\n",
    "        parents,\n",
    "        model_type,\n",
    "        pc_alpha,\n",
    "        threshold,\n",
    "        hidden_layers,\n",
    "        activation,\n",
    "        epochs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : str\n",
    "            Output variable of the model in string format. See Variable.\n",
    "        parents : list(str)\n",
    "            List of strings for the variables that cause the output variable.\n",
    "            See Variable.\n",
    "        model_type : str\n",
    "            # TODO\n",
    "        pc_alpha : str\n",
    "            Meta information. PC alpha used to find the parents.\n",
    "        threshold : str\n",
    "            Meta information. Gridpoint threshold used to select the parents.\n",
    "        hidden_layers : list(int)\n",
    "            Description of the hidden dense layers of the model.\n",
    "        activation : Keras-compatible activation function\n",
    "            Activation function used for the hidden dense layers.\n",
    "        \"\"\"\n",
    "        self.output = Variable.parse_var_name(output)\n",
    "        self.parents = [Variable.parse_var_name(p) for p in parents]\n",
    "        self.model_type = model_type\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.threshold = threshold\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.model = self._build_model()\n",
    "        self.input_vars_dict = ModelDescription._build_vars_dict(self.parents)\n",
    "        self.output_vars_dict = ModelDescription._build_vars_dict([self.output])\n",
    "        \n",
    "        # Training\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Keras model with the given information.\n",
    "        \n",
    "        Some parameters are not configurable, taken from Rasp et al.\n",
    "        \"\"\"\n",
    "        input_shape = len(parents)\n",
    "        input_shape = (input_shape,)\n",
    "        model = dense_nn(\n",
    "            input_shape=input_shape,\n",
    "            output_shape=1,  # Only one output per model\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer = \"adam\", # From train.py (default)\n",
    "            loss = \"mse\", # From 006_8col_pnas_exact.yml\n",
    "            metrics = [tf.keras.losses.mse], # From train.py (default)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_vars_dict(list_variables):\n",
    "        \"\"\"\n",
    "        Convert the given list of Variable into a dictionary to be used\n",
    "        on the data generator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        list_variables : list(Variable)\n",
    "            List of variables to be converted to the dictionary format\n",
    "            used by the data generator\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        vars_dict : dict{str : list(int)}\n",
    "            Dictionary of the form {ds_name : list of levels}, where\n",
    "            \"ds_name\" is the name of the variable as stored in the\n",
    "            dataset, and \"list of levels\" a list containing the indices\n",
    "            of the levels of that variable to use, or None for 2D\n",
    "            variables.\n",
    "        \"\"\"\n",
    "        vars_dict = dict()\n",
    "        for variable in list_variables:\n",
    "            ds_name = variable.var.ds_name # Name used in the dataset\n",
    "            if variable.var.dimensions == 2:\n",
    "                vars_dict[ds_name] = None\n",
    "            elif variable.var.dimensions == 3:\n",
    "                levels = vars_dict.get(ds_name, list())\n",
    "                levels.append(variable.level_idx)\n",
    "                vars_dict[ds_name] = levels\n",
    "        return vars_dict\n",
    "    \n",
    "    def fit_model(self, x, validation_data, callbacks, verbose = 1):\n",
    "        self.model.fit(\n",
    "            x = x,\n",
    "            validation_data = validation_data,\n",
    "            # verbose = 0, # Silent\n",
    "            epochs = self.epochs,\n",
    "            # epochs = 18, # TODO: Decide number of epochs, maybe use early stopper\n",
    "            callbacks = callbacks,\n",
    "            verbose = verbose,\n",
    "        )\n",
    "    \n",
    "    def generate_path(self):\n",
    "        # TODO? Separate \"filename\" from full path, to use in Tensorboard\n",
    "        path = \"models/{model_type}/\".format(\n",
    "            model_type = self.model_type\n",
    "        )\n",
    "        if self.model_type == \"CausalSingleNN\":\n",
    "            path += \"a{pc_alpha}-t{threshold}/\".format(\n",
    "                pc_alpha = self.pc_alpha,\n",
    "                threshold = self.threshold\n",
    "            )\n",
    "        str_hl = str(self.hidden_layers).replace(\", \", \"_\")\n",
    "        str_hl = str_hl.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        path += \"hl_{hidden_layers}-act_{activation}-e_{epochs}/\".format(\n",
    "            hidden_layers = str_hl,\n",
    "            activation = self.activation,\n",
    "            epochs = self.epochs\n",
    "        )\n",
    "        return path\n",
    "    \n",
    "    def generate_filename(self):\n",
    "        output_order = setup.output_order\n",
    "        i_var = setup.output_order.index(self.output.var)\n",
    "        i_level = self.output.level_idx\n",
    "        if i_level is None:\n",
    "            i_level = 0\n",
    "        return f\"{i_var}_{i_level}\"\n",
    "    \n",
    "    def save_model(self):\n",
    "        folder = self.generate_path()\n",
    "        filename = self.generate_filename()\n",
    "        print(f\"Using filename {filename}.\")\n",
    "        # Save model\n",
    "        self.model.save(Path(folder, f\"{filename}_model.h5\"))\n",
    "        # Save weights\n",
    "        self.model.save_weights(Path(folder, f\"{filename}_weights.h5\"))\n",
    "        # Save input list\n",
    "        self.save_input_list(folder, filename)\n",
    "        \n",
    "    def save_input_list(self, folder, filename):\n",
    "        input_list = self.generate_input_list()\n",
    "        with open(Path(folder, f\"{filename}_input_list.txt\"), \"w\") as f:\n",
    "            for line in input_list:\n",
    "                print(str(line), file = f)\n",
    "        \n",
    "    def generate_input_list(self):\n",
    "        # TODO? Move this out of here, to a utils module\n",
    "        inputs = self.parents\n",
    "        input_list = list()\n",
    "        for i_var, spcam_var in enumerate(setup.input_order):\n",
    "            if spcam_var.dimensions == 3:\n",
    "                n_levels = len(setup.levels)\n",
    "            elif spcam_var.dimensions == 2:\n",
    "                n_levels = 1\n",
    "            for i_lvl in range(n_levels):\n",
    "                if spcam_var.dimensions == 3:\n",
    "                    level = setup.levels[i_lvl]\n",
    "                    var = Variable(spcam_var, level, i_lvl)\n",
    "                elif spcam_var.dimensions == 2:\n",
    "                    var = Variable(spcam_var, None, None)\n",
    "                input_list.append(int(var in inputs))\n",
    "        return input_list\n",
    "\n",
    "    def __str__(self):\n",
    "        name = f\"{self.model_type}: {self.output}\"\n",
    "        if self.pc_alpha != None:\n",
    "            # pc_alpha and threshold should be either both None or both not None\n",
    "            name += f\", a{self.pc_alpha}-t{self.threshold}\"\n",
    "        return name\n",
    "#         return self.generate_filename()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(str(self))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return str(self) == str(other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descriptions = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read aggregated results and generate CausalSingleNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, results are loaded from a previously created file. This was\n",
    "made for testing purposes, and we may want to replace it for an analysis.\n",
    "\n",
    "In that replacement, the idea would be to execute more or less the same\n",
    "as aggregate_results up to the moment the file is currently saved, and\n",
    "continue with that object (aggregated results) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Move to configuration\n",
    "HIDDEN_LAYERS = [32, 32, 32]  # Arbitrary, I choose them because it's a small NN\n",
    "ACTIVATION = \"relu\"\n",
    "EPOCHS = 1 # For testing\n",
    "# EPOCHS = 4 # From 006_8col_pnas_exact.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils as utils\n",
    "from pathlib import Path\n",
    "example_aggregated = Path(\n",
    "    \"./aggregated_results\",\n",
    "    setup.yml_filename.replace(\".yml\", \".obj\")\n",
    ")\n",
    "# example_aggregated = \"./aggregated_results/cfg_testing.obj\"\n",
    "aggregated_results = utils.load_results(example_aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SingleNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.do_single_nn:\n",
    "    from utils.constants import SPCAM_Vars\n",
    "    # TODO Only parents & children in configuration\n",
    "    parents = list() # TODO Parents and levels\n",
    "    for spcam_var in setup.var_parents:\n",
    "        if spcam_var.dimensions == 3:\n",
    "            for level, _ in setup.parents_idx_levs:\n",
    "                # There's enough info to build a Variable list\n",
    "                # However, it could be better to do a bigger reorganization\n",
    "                var_name = f\"{spcam_var.name}-{round(level, 2)}\"\n",
    "                parents.append(var_name)\n",
    "        elif spcam_var.dimensions == 2:\n",
    "            var_name = spcam_var.name\n",
    "            parents.append(var_name)\n",
    "    \n",
    "    output_list = list()\n",
    "    for spcam_var in setup.var_children:\n",
    "        if spcam_var.dimensions == 3:\n",
    "            for level, _ in setup.children_idx_levs:\n",
    "                # There's enough info to build a Variable list\n",
    "                # However, it could be better to do a bigger reorganization\n",
    "                var_name = f\"{spcam_var.name}-{round(level, 2)}\"\n",
    "        elif spcam_var.dimensions == 2:\n",
    "            var_name = spcam_var.name\n",
    "        output_list.append(var_name)    \n",
    "\n",
    "    for output in output_list:\n",
    "        model_description = ModelDescription(\n",
    "            output,\n",
    "            parents,\n",
    "            \"SingleNN\",\n",
    "            pc_alpha = None,\n",
    "            threshold = None,\n",
    "            hidden_layers=HIDDEN_LAYERS,\n",
    "            activation=ACTIVATION,\n",
    "            epochs = EPOCHS,\n",
    "        )\n",
    "        model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CausalSingleNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.do_causal_single_nn:\n",
    "    for output, pc_alpha_dict in aggregated_results.items():\n",
    "        print(output)\n",
    "        if len(pc_alpha_dict) == 0:  # May be empty\n",
    "            # TODO How to approach this?\n",
    "            print(\"Empty results\")\n",
    "            pass\n",
    "        for pc_alpha, pc_alpha_results in pc_alpha_dict.items():\n",
    "            var_names = np.array(pc_alpha_results[\"var_names\"])\n",
    "            for threshold, parent_idxs in pc_alpha_results[\"parents\"].items():\n",
    "                parents = var_names[parent_idxs]\n",
    "                model_description = ModelDescription(\n",
    "                    output,\n",
    "                    parents,\n",
    "                    \"CausalSingleNN\",\n",
    "                    pc_alpha,\n",
    "                    threshold,\n",
    "                    hidden_layers=HIDDEN_LAYERS,\n",
    "                    activation=ACTIVATION,\n",
    "                    epochs = EPOCHS,\n",
    "                )\n",
    "                model_descriptions.append(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO? Move this to configuration & constants\n",
    "\n",
    "DATA_TYPE = \"year\"\n",
    "DATA_TYPE = \"month\" # For testing\n",
    "\n",
    "DATA_FOLDER = \"/work/bd1083/b309162/preprocessed_data\"\n",
    "if DATA_TYPE == \"year\":\n",
    "    TRAIN_FN    = \"002_train_1_year.nc\"\n",
    "    VALID_FN    = \"005_valid_1_year.nc\"\n",
    "elif DATA_TYPE == \"month\":\n",
    "    TRAIN_FN    = \"002_train_1_month.nc\"\n",
    "    VALID_FN    = \"000_valid_1_month.nc\"\n",
    "# NORM_FN     = \"000_norm.nc\"\n",
    "NORM_FN_PNAS = (\n",
    "    \"/pf/b/b309198/projects/causal_discovery/rasp-et-al/data/\"\n",
    "    \"001_norm.nc\"\n",
    ") # From Tom https://github.com/tbeucler/CBRAIN-CAM/tree/master/notebooks/tbeucler_devlog/UW_DATA\n",
    "\n",
    "INPUT_SUB = \"mean\" # From 006_8col_pnas_exact.yml\n",
    "INPUT_DIV = \"maxrs\" # From 006_8col_pnas_exact.yml\n",
    "OUT_SCALE_DICT_FN = ( # From 006_8col_pnas_exact.yml\n",
    "    \"/work/bd1179/b309198/causal_discovery/rasp-et-al/CBRAIN-CAM/\"\n",
    "    \"nn_config/scale_dicts/002_pnas_scaling.pkl\" \n",
    ") # From 006_8col_pnas_exact.yml\n",
    "BATCH_SIZE = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.utils import load_pickle\n",
    "out_scale_dict = load_pickle(OUT_SCALE_DICT_FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks.cbrain.data_generator import DataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "def build_train_generator(input_vars_dict, output_vars_dict):\n",
    "    train_gen = DataGenerator(\n",
    "        data_fn          = Path(DATA_FOLDER, TRAIN_FN),\n",
    "        input_vars_dict       = input_vars_dict,\n",
    "        output_vars_dict      = output_vars_dict,\n",
    "        # norm_fn          = Path(DATA_FOLDER, NORM_FN),\n",
    "        norm_fn          = NORM_FN_PNAS,\n",
    "        input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "        output_transform = out_scale_dict,\n",
    "        batch_size       = BATCH_SIZE,\n",
    "        shuffle          = True, # This feature doesn't seem to work\n",
    "    )\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "nlat=64\n",
    "nlon=128\n",
    "ngeo = nlat * nlon\n",
    "\n",
    "def build_valid_generator(input_vars_dict, output_vars_dict):\n",
    "    valid_gen = DataGenerator(\n",
    "            data_fn          = Path(DATA_FOLDER, VALID_FN),\n",
    "            input_vars_dict       = input_vars_dict,\n",
    "            output_vars_dict      = output_vars_dict,\n",
    "            norm_fn          = NORM_FN_PNAS,\n",
    "            input_transform  = (INPUT_SUB, INPUT_DIV),\n",
    "            output_transform = out_scale_dict,\n",
    "            batch_size       = ngeo,\n",
    "            shuffle          = False,\n",
    "            #xarray           = True,\n",
    "    )\n",
    "    return valid_gen\n",
    "\n",
    "\n",
    "from neural_networks.cbrain.save_weights import save_norm\n",
    "# TODO Save input and output transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is only for developing purposes, to not have to restart the full\n",
    "Notebook every time one of these files was changed.\n",
    "\n",
    "Can be safely removed once they are stable.\n",
    "\"\"\"\n",
    "from importlib import reload\n",
    "import neural_networks.cbrain.data_generator\n",
    "import neural_networks.cbrain.normalization\n",
    "import neural_networks.cbrain.utils\n",
    "import neural_networks.cbrain.learning_rate_schedule\n",
    "reload(neural_networks.cbrain.data_generator)\n",
    "reload(neural_networks.cbrain.utils)\n",
    "reload(neural_networks.cbrain.normalization)\n",
    "reload(neural_networks.cbrain.learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all models in the model list and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from neural_networks.cbrain.learning_rate_schedule import LRUpdate\n",
    "\n",
    "for model_description in model_descriptions:\n",
    "    print(model_description)\n",
    "    \n",
    "    input_vars_dict = model_description.input_vars_dict\n",
    "    output_vars_dict = model_description.output_vars_dict\n",
    "    \n",
    "    # TODO Test that the data is being taken correctly\n",
    "    with build_train_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as train_gen, build_valid_generator(\n",
    "        input_vars_dict, output_vars_dict\n",
    "    ) as valid_gen:\n",
    "        lrs = LearningRateScheduler(LRUpdate(\n",
    "            init_lr = 0.001, # From train.py (default)\n",
    "            step = 1, # From 006_8col_pnas_exact.yml\n",
    "            divide = 5 # From train.py (default)\n",
    "        ))\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f\"tensorboard/{model_description}\", # TODO configure folder\n",
    "            histogram_freq=0,\n",
    "            write_graph=True,\n",
    "            write_images=False,\n",
    "            update_freq=\"epoch\",\n",
    "            profile_batch=2,\n",
    "            embeddings_freq=0,\n",
    "            embeddings_metadata=None,\n",
    "        )\n",
    "\n",
    "        model_description.fit_model(\n",
    "            x = train_gen,\n",
    "            validation_data = valid_gen,\n",
    "            callbacks = [lrs, tensorboard],\n",
    "            # verbose = 0, # Silent\n",
    "#             verbose = 2, # Summary on epoch\n",
    "        )\n",
    "        model_description.save_model()\n",
    "#         print(len(train_gen))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalNNCAM",
   "language": "python",
   "name": "causalnncam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
