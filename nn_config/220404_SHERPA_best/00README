History:
	- 2022-Apr-04: Written, FIS

Description:
	- This 00README file describes how the "best" SHERPA case was chosen.

Procedure (followed for each output):
	The complexity of the NN (num. of trainable parameters) are primarily
	determined by the num. of nodes, and secondary by the num. of hidden
	layers. The procedure described below is motivated by the fact that 
	NNs are parameters "hungry", i.e., in general the more trainable 
	parameters the smaller validation loss.  
	
	- SHERPA trials and loss (objective). For each num. of nodes it was 
	  chosen the "best" case based on the num. of layers, i.e., shallowest
	  model that achieve the lowest loss (objective) using two significant 
	  figures.

	- Evaluation (R2 based on the test set). Best cases for each num. of
	  nodes was evaluated, and the SHERPA best case results from choosing
	  the simplest model (fewest num. of trainable parameters) that achieves
	  the greatest R2 value using two significant figures.
	  => i.e., see notebooks_evaluate_SHERPA_cases_r2/
	           evaluate_SHERPA_cases.ipynb; NN_num_parameters.ipynb
		    
	  