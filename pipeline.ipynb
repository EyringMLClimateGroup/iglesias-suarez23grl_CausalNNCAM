{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***DESCRIPTION*** \n",
    "## ***Run Tigramite (PCMCI) for SPCAM data with specified settings:***\n",
    "### Fixed:\n",
    "- PC-stable (i.e., MCI component not run)\n",
    "- tau_min/tau_max = -1\n",
    "- Significance: analytics\n",
    "- experiments: '002_train_1_year'\n",
    "- links: parents (state fields) -> children (parameterizations)\n",
    "### Options:\n",
    "- analysis: 'single': gridpoints individually\n",
    "            'concat': gridpoints contatenated into a \n",
    "                      single time-series\n",
    "- children (parameterizations)\n",
    "- region: lat/lon limits (gridpoints to be used)\n",
    "- levels: children's levels to be explored\n",
    "- pc_alphas: list of value(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import sys, getopt, yaml, time, datetime\n",
    "from datetime import datetime as dt\n",
    "import numpy                  as np\n",
    "from pathlib              import Path\n",
    "\n",
    "# Utils\n",
    "from   utils.constants    import SPCAM_Vars, DATA_FOLDER, ANCIL_FILE #, OUTPUT_FILE_PATTERN\n",
    "from   utils.constants    import tau_min, tau_max, significance, experiment\n",
    "import utils.utils            as utils\n",
    "import utils.processing       as proc\n",
    "# import utils.links            as links\n",
    "import utils.pcmci_algorithm  as algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv           = sys.argv[1:] # argv           = ['-c', 'cfg_pipeline.yml']\n",
    "#argv           = ['-c', 'cfg_pipeline.yml']\n",
    "try:\n",
    "    opts, args = getopt.getopt(argv,\"hc:a\",[\"cfg_file=\",\"add=\"])\n",
    "except getopt.GetoptError:\n",
    "    print ('pipeline.py -c [cfg_file] -a [add]')\n",
    "    sys.exit(2)\n",
    "for opt, arg in opts:\n",
    "    if opt == '-h':\n",
    "        print ('pipeline.py -c [cfg_file]')\n",
    "        sys.exit()\n",
    "    elif opt in (\"-c\", \"--cfg_file\"):\n",
    "        yml_cfgFilenm = arg\n",
    "    elif opt in (\"-a\", \"--add\"):\n",
    "        pass\n",
    "\n",
    "# YAML config file\n",
    "yml_cfgFile       = open(yml_cfgFilenm)\n",
    "yml_cfg           = yaml.load(yml_cfgFile, Loader=yaml.FullLoader)\n",
    "\n",
    "# Load specifications\n",
    "analysis            = yml_cfg['analysis']\n",
    "spcam_parents       = yml_cfg['spcam_parents']\n",
    "spcam_children      = yml_cfg['spcam_children']\n",
    "pc_alphas           = yml_cfg['pc_alphas']\n",
    "region              = yml_cfg['region']\n",
    "lim_levels          = yml_cfg['lim_levels']\n",
    "target_levels       = yml_cfg['target_levels']\n",
    "verbosity           = yml_cfg['verbosity']\n",
    "output_folder       = yml_cfg['output_folder']\n",
    "output_file_pattern = yml_cfg['output_file_pattern'][analysis]\n",
    "overwrite           = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Region / Gridpoints\n",
    "if region is False:\n",
    "    region     = [ [-90,90] , [0,-.5] ] # All\n",
    "gridpoints = utils.get_gridpoints(region)\n",
    "\n",
    "## Children levels (parents includes all)\n",
    "if lim_levels is not False and target_levels is False:\n",
    "    target_levels = utils.get_levels(lim_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model's grid\n",
    "levels, latitudes, longitudes = utils.read_ancilaries(Path(DATA_FOLDER, ANCIL_FILE))\n",
    "\n",
    "## Latitude / Longitude indexes\n",
    "idx_lats = [utils.find_closest_value(latitudes, gridpoint[0])      for gridpoint in gridpoints]\n",
    "idx_lons = [utils.find_closest_longitude(longitudes, gridpoint[1]) for gridpoint in gridpoints]\n",
    "\n",
    "## Level indexes (children & parents)\n",
    "parents_idx_levs = [[round(lev, 2), i] for i, lev in enumerate(levels)] # All\n",
    "if target_levels is not False:\n",
    "    children_idx_levs = [[lev, utils.find_closest_value(levels, lev)] for lev in target_levels]\n",
    "else:\n",
    "    children_idx_levs = parents_idx_levs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "spcam_vars_include = spcam_parents + spcam_children\n",
    "SPCAM_Vars         = [var for var in SPCAM_Vars if var.label in spcam_vars_include]\n",
    "var_parents        = [var for var in SPCAM_Vars if var.type == \"in\"]\n",
    "var_children       = [var for var in SPCAM_Vars if var.type == \"out\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.185920533189154, 120.9375]]\n",
      "[(tbp, 3, in), (ps, 2, in)]\n",
      "[(fsns, 2, out)]\n",
      "[[3.64, 0], [7.59, 1], [14.36, 2], [24.61, 3], [38.27, 4], [54.6, 5], [72.01, 6], [87.82, 7], [103.32, 8], [121.55, 9], [142.99, 10], [168.23, 11], [197.91, 12], [232.83, 13], [273.91, 14], [322.24, 15], [379.1, 16], [445.99, 17], [524.69, 18], [609.78, 19], [691.39, 20], [763.4, 21], [820.86, 22], [859.53, 23], [887.02, 24], [912.64, 25], [936.2, 26], [957.49, 27], [976.33, 28], [992.56, 29]]\n",
      "[[3.64, 0], [7.59, 1], [14.36, 2], [24.61, 3], [38.27, 4], [54.6, 5], [72.01, 6], [87.82, 7], [103.32, 8], [121.55, 9], [142.99, 10], [168.23, 11], [197.91, 12], [232.83, 13], [273.91, 14], [322.24, 15], [379.1, 16], [445.99, 17], [524.69, 18], [609.78, 19], [691.39, 20], [763.4, 21], [820.86, 22], [859.53, 23], [887.02, 24], [912.64, 25], [936.2, 26], [957.49, 27], [976.33, 28], [992.56, 29]]\n"
     ]
    }
   ],
   "source": [
    "print(gridpoints)\n",
    "print(var_parents)\n",
    "print(var_children)\n",
    "print(parents_idx_levs)\n",
    "print(children_idx_levs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(\n",
    "    gridpoints,\n",
    "    var_parents,\n",
    "    var_children,\n",
    "    pc_alphas,\n",
    "    parents_idx_levs,\n",
    "    children_idx_levs,\n",
    "    idx_lats,\n",
    "    idx_lons,\n",
    "    output_file_pattern,\n",
    "    output_folder,\n",
    "    overwrite\n",
    "          ):\n",
    "    \n",
    "    ## Model's grid\n",
    "    levels, latitudes, longitudes = utils.read_ancilaries(Path(DATA_FOLDER, ANCIL_FILE))\n",
    "    \n",
    "    ## Processing\n",
    "    len_grid     = len(gridpoints)\n",
    "    t_start      = time.time()\n",
    "    data_parents = None\n",
    "    \n",
    "    ## outFile exists?\n",
    "    for child in var_children:\n",
    "        print(f\"{dt.now()} Variable: {child.name}\")\n",
    "        if child.dimensions == 2:\n",
    "            child_levels = [[levels[-1],0]]\n",
    "        elif child.dimensions == 3:\n",
    "            child_levels = children_idx_levs\n",
    "        for level in child_levels:\n",
    "            \n",
    "            results_filename = output_file_pattern.format(\n",
    "                        var_name   = child.name,\n",
    "                        level      = level[-1]+1,\n",
    "                        lat1       = int(gridpoints[0][0]),\n",
    "                        lat2       = int(gridpoints[-1][0]),\n",
    "                        lon1       = int(gridpoints[0][-1]),\n",
    "                        lon2       = int(gridpoints[-1][-1]),\n",
    "                        experiment = experiment\n",
    "                )\n",
    "            results_file = Path(output_folder, results_filename)\n",
    "    \n",
    "    \n",
    "            if not overwrite and results_file.is_file():\n",
    "                print(f\"{dt.now()} Found file {results_file}, skipping.\")\n",
    "                continue # Ignore this level\n",
    "    \n",
    "\n",
    "            # Only load parents if necessary to analyze a child\n",
    "            # they stay loaded until the next gridpoint\n",
    "            if data_parents is None:\n",
    "                print(); print(f\"Load Parents (state fields)...\")\n",
    "                t_before_load_parents = time.time()\n",
    "                for i_grid, (i_lat, i_lon) in enumerate(gridpoints):\n",
    "\n",
    "                    t_start_gridpoint = time.time()\n",
    "\n",
    "                    idx_lat = idx_lats[i_grid]\n",
    "                    idx_lon = idx_lons[i_grid]\n",
    "                \n",
    "                    print(f\"{dt.now()} Gridpoint {i_grid+1}/{len_grid}: lat={latitudes[idx_lats[i_grid]]}\"\n",
    "                          + f\" ({idx_lat}), lon={longitudes[idx_lons[i_grid]]} ({idx_lon})\")\n",
    "\n",
    "                    normalized_parents = utils.load_data_concat(\n",
    "                        var_parents,\n",
    "                        experiment,\n",
    "                        DATA_FOLDER,\n",
    "                        parents_idx_levs,\n",
    "                        idx_lat,\n",
    "                        idx_lon)\n",
    "                    if data_parents is None:\n",
    "                        data_parents = normalized_parents\n",
    "                    else:\n",
    "                        data_parents = np.concatenate((data_parents, normalized_parents), axis=1)\n",
    "                # Format data\n",
    "                data_parents = utils.format_data(data_parents, var_parents, parents_idx_levs)\n",
    "\n",
    "                time_load_parents = datetime.timedelta(seconds = time.time() - t_before_load_parents)\n",
    "                print(f\"{dt.now()} All parents loaded. Time: {time_load_parents}\"); print(\"\")\n",
    "            \n",
    "            \n",
    "            # Process data child\n",
    "            print(f\"Load {child.name}...\")\n",
    "            t_before_load_child = time.time()\n",
    "            data_child = None\n",
    "            for i_grid, (i_lat, i_lon) in enumerate(gridpoints):\n",
    "                \n",
    "                idx_lat = idx_lats[i_grid]\n",
    "                idx_lon = idx_lons[i_grid]\n",
    "            \n",
    "                normalized_child = utils.load_data_concat(\n",
    "                        [child],\n",
    "                        experiment,\n",
    "                        DATA_FOLDER,\n",
    "                        [level],\n",
    "                        idx_lat,\n",
    "                        idx_lon)\n",
    "                if data_child is None:\n",
    "                    data_child = normalized_child\n",
    "                else:\n",
    "                    data_child = np.concatenate((data_child, normalized_child), axis=1)\n",
    "            time_load_child = datetime.timedelta(seconds = time.time() - t_before_load_child)\n",
    "            print(f\"{dt.now()} Child loaded. Time: {time_load_child}\"); print(\"\")\n",
    "            \n",
    "            # Format data\n",
    "            data_child = utils.format_data(data_child, [child], [level])\n",
    "            data = [*data_parents, *data_child]\n",
    "            \n",
    "            # Find links\n",
    "            print(f\"{dt.now()} Finding links for {child.name} at level {level[-1]+1}\")\n",
    "            t_before_find_links = time.time()\n",
    "            results = algorithm.find_links(data, pc_alphas, 0)\n",
    "            time_links = datetime.timedelta(seconds = time.time() - t_before_find_links)\n",
    "            total_time = datetime.timedelta(seconds = time.time() - t_start)\n",
    "            print(f\"{dt.now()} Links found. Time: {time_links}\" + f\" Total time so far: {total_time}\")\n",
    "            print()\n",
    "            \n",
    "            # Store causal links\n",
    "            utils.save_results(results, results_filename, output_folder)\n",
    "\n",
    "\n",
    "    total_time = datetime.timedelta(seconds = time.time() - t_start)\n",
    "    print(f\"{dt.now()} Execution complete. Total time: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-22 17:53:47.755003 Variable: fsns\n",
      "2021-02-22 17:53:47.755476 Gridpoint 1/1: lat=4.185920533189154 (33), lon=120.9375 (43)\n",
      "Load Parents (state fields)...\n",
      "2021-02-22 17:54:09.268938 All parents loaded. Time: 0:00:21.513329\n",
      "2021-02-22 17:54:09.952248 Finding links for fsns at level 1\n",
      "2021-02-22 17:54:16.025105 Links found. Time: 0:00:06.072633 Total time so far: 0:00:28.270103\n",
      "Saved results into \"test_causal_links/fsns_1_lat-4_lon-120_002_train_1_year.obj\"\n",
      "2021-02-22 17:54:16.029378 All links in gridpoint found. Time: 0:00:28.274368. Total time so far: 0:00:28.274376\n",
      "\n",
      "2021-02-22 17:54:16.029431 Execution complete. Total time: 0:00:28.274376\n"
     ]
    }
   ],
   "source": [
    "if analysis == 'single':\n",
    "    proc.single(\n",
    "        gridpoints,\n",
    "        var_parents,\n",
    "        var_children,\n",
    "        pc_alphas,\n",
    "        parents_idx_levs,\n",
    "        children_idx_levs,\n",
    "        idx_lats,\n",
    "        idx_lons,\n",
    "        output_file_pattern,\n",
    "        output_folder,\n",
    "        overwrite\n",
    "    )\n",
    "elif analysis == 'concat':\n",
    "    concat(\n",
    "        gridpoints,\n",
    "        var_parents,\n",
    "        var_children,\n",
    "        pc_alphas,\n",
    "        parents_idx_levs,\n",
    "        children_idx_levs,\n",
    "        idx_lats,\n",
    "        idx_lons,\n",
    "        output_file_pattern,\n",
    "        output_folder,\n",
    "        overwrite\n",
    "    )\n",
    "else:\n",
    "    print(\"Please specify a valid analysis, i.e., 'single' or 'concat'; stop script\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tg38plus",
   "language": "python",
   "name": "tg38plus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
